2022-11-02 05:23:32 | INFO | fairseq.distributed.utils | distributed init (rank 1): tcp://localhost:14878
2022-11-02 05:23:32 | INFO | fairseq.distributed.utils | distributed init (rank 0): tcp://localhost:14878
2022-11-02 05:23:32 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 1
2022-11-02 05:23:32 | INFO | fairseq.distributed.utils | distributed init (rank 3): tcp://localhost:14878
2022-11-02 05:23:32 | INFO | fairseq.distributed.utils | distributed init (rank 2): tcp://localhost:14878
2022-11-02 05:23:32 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 3
2022-11-02 05:23:32 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 2
2022-11-02 05:23:32 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 0
2022-11-02 05:23:32 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
2022-11-02 05:23:32 | INFO | fairseq.distributed.utils | initialized host scn29-mn as rank 0
2022-11-02 05:23:32 | INFO | torch.distributed.distributed_c10d | Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
2022-11-02 05:23:32 | INFO | fairseq.distributed.utils | initialized host scn29-mn as rank 1
2022-11-02 05:23:32 | INFO | torch.distributed.distributed_c10d | Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
2022-11-02 05:23:32 | INFO | fairseq.distributed.utils | initialized host scn29-mn as rank 3
2022-11-02 05:23:32 | INFO | torch.distributed.distributed_c10d | Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
2022-11-02 05:23:32 | INFO | fairseq.distributed.utils | initialized host scn29-mn as rank 2
2022-11-02 05:23:45 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': 'Indic-En-Distillation', 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': True, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': 'indicTrans/model_configs', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 4, 'distributed_num_procs': 4, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:14878', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 4, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': True, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 64, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 16384, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 16384, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 1000000, 'stop_time_hours': 0.0, 'clip_norm': 1.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints/global-multi-distil', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': 1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': 5, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 4}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress_bar=False, log_interval=100, log_format=None, log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir=None, wandb_project='Indic-En-Distillation', azureml_logging=False, seed=1, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=True, memory_efficient_fp16=True, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir='indicTrans/model_configs', empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='label_smoothed_cross_entropy_with_kd', tokenizer=None, bpe=None, optimizer='adam', lr_scheduler='inverse_sqrt', scoring='bleu', task='translation', num_workers=64, skip_invalid_size_inputs_valid_test=True, max_tokens=16384, batch_size=None, required_batch_size_multiple=8, required_seq_len_multiple=1, dataset_impl=None, data_buffer_size=10, train_subset='train', valid_subset='valid', combine_valid_subsets=None, ignore_unused_valid_subsets=False, validate_interval=1, validate_interval_updates=0, validate_after_updates=0, fixed_validation_seed=None, disable_validation=False, max_tokens_valid=16384, batch_size_valid=None, max_valid_steps=None, curriculum=0, gen_subset='test', num_shards=1, shard_id=0, grouped_shuffling=False, update_epoch_batch_itr=False, update_ordered_indices_seed=False, distributed_world_size=4, distributed_num_procs=4, distributed_rank=0, distributed_backend='nccl', distributed_init_method=None, distributed_port=-1, device_id=0, distributed_no_spawn=False, ddp_backend='pytorch_ddp', ddp_comm_hook='none', bucket_cap_mb=25, fix_batches_to_gpus=False, find_unused_parameters=False, gradient_as_bucket_view=False, fast_stat_sync=False, heartbeat_timeout=-1, broadcast_buffers=False, slowmo_momentum=None, slowmo_base_algorithm='localsgd', localsgd_frequency=3, nprocs_per_node=4, pipeline_model_parallel=False, pipeline_balance=None, pipeline_devices=None, pipeline_chunks=0, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_checkpoint='never', zero_sharding='none', no_reshard_after_forward=False, fp32_reduce_scatter=False, cpu_offload=False, use_sharded_state=False, not_fsdp_flatten_parameters=False, arch='transformer', max_epoch=0, max_update=1000000, stop_time_hours=0, clip_norm=1.0, sentence_avg=False, update_freq=[1], lr=[0.0005], stop_min_lr=-1.0, use_bmuf=False, skip_remainder_batch=False, save_dir='checkpoints/global-multi-distil', restore_file='checkpoint_last.pt', continue_once=None, finetune_from_model=None, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimizer_overrides='{}', save_interval=1, save_interval_updates=0, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=1, keep_best_checkpoints=-1, no_save=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_save_optimizer_state=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, patience=5, checkpoint_suffix='', checkpoint_shard_count=1, load_checkpoint_on_all_dp_ranks=False, write_checkpoints_asynchronously=False, store_ema=False, ema_decay=0.9999, ema_start_update=0, ema_seed_model=None, ema_update_freq=1, ema_fp32=False, data='indic-en-exp/final_bin', source_lang='SRC', target_lang='TGT', load_alignments=False, left_pad_source=True, left_pad_target=False, upsample_primary=-1, truncate_source=False, num_batch_buckets=0, eval_bleu=False, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_tokenized_bleu=False, eval_bleu_remove_bpe=None, eval_bleu_print_samples=False, kd_strategy='global_multi_level', teacher_checkpoint_path='checkpoints/indicTrans/checkpoint_best.pt', label_smoothing=0.1, report_accuracy=False, ignore_prefix_size=0, kd_rate=0.5, kd_queue_size=50000, student_temp=1, teacher_temp=1, alpha=0.5, use_adaptive_weightage=False, adaptive_smoothing=None, use_adaptive_kd_rates=False, kd_selection_temp=None, adam_betas='(0.9, 0.98)', adam_eps=1e-08, weight_decay=0.0, use_old_adam=False, fp16_adam_stats=False, warmup_updates=4000, warmup_init_lr=1e-07, pad=1, eos=2, unk=3, max_source_positions=210, max_target_positions=210, activation_fn='gelu', layernorm_embedding=True, decoder_normalize_before=True, encoder_normalize_before=True, dropout=0.2, no_seed_provided=False, encoder_embed_path=None, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_attention_heads=8, encoder_learned_pos=False, decoder_embed_path=None, decoder_embed_dim=512, decoder_ffn_embed_dim=2048, decoder_layers=6, decoder_attention_heads=8, decoder_learned_pos=False, attention_dropout=0.0, activation_dropout=0.0, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, share_decoder_input_output_embed=False, share_all_embeddings=False, merge_src_tgt_embed=False, no_token_positional_embeddings=False, adaptive_input=False, no_cross_attention=False, cross_self_attention=False, decoder_output_dim=512, decoder_input_dim=512, no_scale_embedding=False, tie_adaptive_weights=False, checkpoint_activations=False, offload_activations=False, encoder_layers_to_keep=None, decoder_layers_to_keep=None, encoder_layerdrop=0, decoder_layerdrop=0, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, _name='transformer'), 'task': {'_name': 'translation', 'data': 'indic-en-exp/final_bin', 'source_lang': 'SRC', 'target_lang': 'TGT', 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 210, 'max_target_positions': 210, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False, 'kd_strategy': 'global_multi_level', 'teacher_checkpoint_path': 'checkpoints/indicTrans/checkpoint_best.pt'}, 'criterion': {'_name': 'label_smoothed_cross_entropy_with_kd', 'label_smoothing': 0.1, 'report_accuracy': False, 'ignore_prefix_size': 0, 'kd_rate': 0.5, 'kd_queue_size': 50000, 'student_temp': 1.0, 'teacher_temp': 1.0, 'alpha': 0.5, 'use_adaptive_weightage': False, 'adaptive_smoothing': None, 'use_adaptive_kd_rates': False, 'kd_selection_temp': None, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.0, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': 1e-07, 'lr': [0.0005]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}
2022-11-02 05:23:45 | INFO | fairseq.tasks.translation | [SRC] dictionary: 35904 types
2022-11-02 05:23:45 | INFO | fairseq.tasks.translation | [TGT] dictionary: 32088 types
2022-11-02 05:23:47 | INFO | fairseq_cli.train | TransformerModel(
  (encoder): TransformerEncoderBase(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(35904, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layernorm_embedding): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TransformerDecoderBase(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(32088, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layernorm_embedding): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
    (output_projection): Linear(in_features=512, out_features=32088, bias=False)
  )
)
2022-11-02 05:23:47 | INFO | fairseq_cli.train | task: TranslationTask
2022-11-02 05:23:47 | INFO | fairseq_cli.train | model: TransformerModel
2022-11-02 05:23:47 | INFO | fairseq_cli.train | criterion: KDLabelSmoothedCrossEntropyCriterion
2022-11-02 05:23:47 | INFO | fairseq_cli.train | num. shared model params: 95,383,552 (num. trained: 95,383,552)
2022-11-02 05:23:47 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2022-11-02 05:23:47 | INFO | fairseq.data.data_utils | loaded 78,941 examples from: indic-en-exp/final_bin/valid.SRC-TGT.SRC
2022-11-02 05:23:47 | INFO | fairseq.data.data_utils | loaded 78,941 examples from: indic-en-exp/final_bin/valid.SRC-TGT.TGT
2022-11-02 05:23:47 | INFO | fairseq.tasks.translation | indic-en-exp/final_bin valid SRC-TGT 78941 examples
2022-11-02 05:23:51 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:2 to store for rank: 0
2022-11-02 05:23:51 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:2 with 4 nodes.
2022-11-02 05:23:54 | INFO | fairseq.utils | ***********************CUDA enviroments for all 4 workers***********************
2022-11-02 05:23:54 | INFO | fairseq.utils | rank   0: capabilities =  8.0  ; total memory = 39.587 GB ; name = A100-SXM4-40GB                          
2022-11-02 05:23:54 | INFO | fairseq.utils | rank   1: capabilities =  8.0  ; total memory = 39.587 GB ; name = A100-SXM4-40GB                          
2022-11-02 05:23:54 | INFO | fairseq.utils | rank   2: capabilities =  8.0  ; total memory = 39.587 GB ; name = A100-SXM4-40GB                          
2022-11-02 05:23:54 | INFO | fairseq.utils | rank   3: capabilities =  8.0  ; total memory = 39.587 GB ; name = A100-SXM4-40GB                          
2022-11-02 05:23:54 | INFO | fairseq.utils | ***********************CUDA enviroments for all 4 workers***********************
2022-11-02 05:23:54 | INFO | fairseq_cli.train | training on 4 devices (GPUs/TPUs)
2022-11-02 05:23:54 | INFO | fairseq_cli.train | max tokens per device = 16384 and max sentences per device = None
2022-11-02 05:23:54 | INFO | fairseq.trainer | Preparing to load checkpoint checkpoints/global-multi-distil/checkpoint_last.pt
2022-11-02 05:24:02 | INFO | fairseq.optim.adam | using FusedAdam
2022-11-02 05:24:20 | INFO | fairseq.trainer | Loaded checkpoint checkpoints/global-multi-distil/checkpoint_last.pt (epoch 15 @ 224342 updates)
2022-11-02 05:24:20 | INFO | fairseq.trainer | loading train data for epoch 15
2022-11-02 05:24:21 | INFO | fairseq.data.data_utils | loaded 49,772,565 examples from: indic-en-exp/final_bin/train.SRC-TGT.SRC
2022-11-02 05:24:21 | INFO | fairseq.data.data_utils | loaded 49,772,565 examples from: indic-en-exp/final_bin/train.SRC-TGT.TGT
2022-11-02 05:24:21 | INFO | fairseq.tasks.translation | indic-en-exp/final_bin train SRC-TGT 49772565 examples
2022-11-02 05:24:35 | WARNING | fairseq.tasks.fairseq_task | 11,612 samples have invalid sizes and will be skipped, max_positions=(210, 210), first few sample ids=[38134819, 6246747, 2525593, 1919047, 33622048, 7799562, 4508869, 12466026, 14778728, 20258840]
2022-11-02 05:24:36 | WARNING | fairseq.tasks.fairseq_task | 11,612 samples have invalid sizes and will be skipped, max_positions=(210, 210), first few sample ids=[38134819, 6246747, 2525593, 1919047, 33622048, 7799562, 4508869, 12466026, 14778728, 20258840]
2022-11-02 05:24:36 | WARNING | fairseq.tasks.fairseq_task | 11,612 samples have invalid sizes and will be skipped, max_positions=(210, 210), first few sample ids=[38134819, 6246747, 2525593, 1919047, 33622048, 7799562, 4508869, 12466026, 14778728, 20258840]
2022-11-02 05:24:36 | WARNING | fairseq.tasks.fairseq_task | 11,612 samples have invalid sizes and will be skipped, max_positions=(210, 210), first few sample ids=[38134819, 6246747, 2525593, 1919047, 33622048, 7799562, 4508869, 12466026, 14778728, 20258840]
2022-11-02 05:25:01 | INFO | fairseq_cli.train | loaded teacher model TransformerModel(
  (encoder): TransformerEncoderBase(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(35904, 1536, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1536, out_features=1536, bias=True)
          (v_proj): Linear(in_features=1536, out_features=1536, bias=True)
          (q_proj): Linear(in_features=1536, out_features=1536, bias=True)
          (out_proj): Linear(in_features=1536, out_features=1536, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([1536]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1536, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1536, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([1536]), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1536, out_features=1536, bias=True)
          (v_proj): Linear(in_features=1536, out_features=1536, bias=True)
          (q_proj): Linear(in_features=1536, out_features=1536, bias=True)
          (out_proj): Linear(in_features=1536, out_features=1536, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([1536]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1536, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1536, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([1536]), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1536, out_features=1536, bias=True)
          (v_proj): Linear(in_features=1536, out_features=1536, bias=True)
          (q_proj): Linear(in_features=1536, out_features=1536, bias=True)
          (out_proj): Linear(in_features=1536, out_features=1536, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([1536]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1536, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1536, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([1536]), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1536, out_features=1536, bias=True)
          (v_proj): Linear(in_features=1536, out_features=1536, bias=True)
          (q_proj): Linear(in_features=1536, out_features=1536, bias=True)
          (out_proj): Linear(in_features=1536, out_features=1536, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([1536]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1536, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1536, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([1536]), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1536, out_features=1536, bias=True)
          (v_proj): Linear(in_features=1536, out_features=1536, bias=True)
          (q_proj): Linear(in_features=1536, out_features=1536, bias=True)
          (out_proj): Linear(in_features=1536, out_features=1536, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([1536]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1536, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1536, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([1536]), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1536, out_features=1536, bias=True)
          (v_proj): Linear(in_features=1536, out_features=1536, bias=True)
          (q_proj): Linear(in_features=1536, out_features=1536, bias=True)
          (out_proj): Linear(in_features=1536, out_features=1536, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([1536]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1536, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1536, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([1536]), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (decoder): TransformerDecoderBase(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(32088, 1536, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1536, out_features=1536, bias=True)
          (v_proj): Linear(in_features=1536, out_features=1536, bias=True)
          (q_proj): Linear(in_features=1536, out_features=1536, bias=True)
          (out_proj): Linear(in_features=1536, out_features=1536, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([1536]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1536, out_features=1536, bias=True)
          (v_proj): Linear(in_features=1536, out_features=1536, bias=True)
          (q_proj): Linear(in_features=1536, out_features=1536, bias=True)
          (out_proj): Linear(in_features=1536, out_features=1536, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([1536]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1536, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1536, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([1536]), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1536, out_features=1536, bias=True)
          (v_proj): Linear(in_features=1536, out_features=1536, bias=True)
          (q_proj): Linear(in_features=1536, out_features=1536, bias=True)
          (out_proj): Linear(in_features=1536, out_features=1536, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([1536]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1536, out_features=1536, bias=True)
          (v_proj): Linear(in_features=1536, out_features=1536, bias=True)
          (q_proj): Linear(in_features=1536, out_features=1536, bias=True)
          (out_proj): Linear(in_features=1536, out_features=1536, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([1536]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1536, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1536, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([1536]), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1536, out_features=1536, bias=True)
          (v_proj): Linear(in_features=1536, out_features=1536, bias=True)
          (q_proj): Linear(in_features=1536, out_features=1536, bias=True)
          (out_proj): Linear(in_features=1536, out_features=1536, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([1536]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1536, out_features=1536, bias=True)
          (v_proj): Linear(in_features=1536, out_features=1536, bias=True)
          (q_proj): Linear(in_features=1536, out_features=1536, bias=True)
          (out_proj): Linear(in_features=1536, out_features=1536, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([1536]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1536, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1536, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([1536]), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1536, out_features=1536, bias=True)
          (v_proj): Linear(in_features=1536, out_features=1536, bias=True)
          (q_proj): Linear(in_features=1536, out_features=1536, bias=True)
          (out_proj): Linear(in_features=1536, out_features=1536, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([1536]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1536, out_features=1536, bias=True)
          (v_proj): Linear(in_features=1536, out_features=1536, bias=True)
          (q_proj): Linear(in_features=1536, out_features=1536, bias=True)
          (out_proj): Linear(in_features=1536, out_features=1536, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([1536]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1536, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1536, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([1536]), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1536, out_features=1536, bias=True)
          (v_proj): Linear(in_features=1536, out_features=1536, bias=True)
          (q_proj): Linear(in_features=1536, out_features=1536, bias=True)
          (out_proj): Linear(in_features=1536, out_features=1536, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([1536]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1536, out_features=1536, bias=True)
          (v_proj): Linear(in_features=1536, out_features=1536, bias=True)
          (q_proj): Linear(in_features=1536, out_features=1536, bias=True)
          (out_proj): Linear(in_features=1536, out_features=1536, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([1536]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1536, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1536, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([1536]), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1536, out_features=1536, bias=True)
          (v_proj): Linear(in_features=1536, out_features=1536, bias=True)
          (q_proj): Linear(in_features=1536, out_features=1536, bias=True)
          (out_proj): Linear(in_features=1536, out_features=1536, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([1536]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1536, out_features=1536, bias=True)
          (v_proj): Linear(in_features=1536, out_features=1536, bias=True)
          (q_proj): Linear(in_features=1536, out_features=1536, bias=True)
          (out_proj): Linear(in_features=1536, out_features=1536, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([1536]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1536, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1536, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([1536]), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=1536, out_features=32088, bias=False)
  )
) from checkpoints/indicTrans/checkpoint_best.pt in evaluation mode
2022-11-02 05:25:01 | INFO | fairseq.data.iterators | grouped total_num_itrs = 16028
2022-11-02 05:25:09 | INFO | fairseq.trainer | begin training epoch 15
2022-11-02 05:25:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-11-02 05:26:05 | WARNING | urllib3.connectionpool | Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProxyError('Cannot connect to proxy.', OSError('Tunnel connection failed: 403 Forbidden'))': /api/5288891/envelope/
2022-11-02 05:26:05 | WARNING | urllib3.connectionpool | Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProxyError('Cannot connect to proxy.', OSError('Tunnel connection failed: 403 Forbidden'))': /api/5288891/envelope/
2022-11-02 05:26:05 | WARNING | urllib3.connectionpool | Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProxyError('Cannot connect to proxy.', OSError('Tunnel connection failed: 403 Forbidden'))': /api/5288891/envelope/
2022-11-02 05:32:26 | INFO | torch.nn.parallel.distributed | Reducer buckets have been rebuilt in this iteration.
2022-11-02 05:33:45 | INFO | train_inner | epoch 015:     58 / 16028 loss=1.615, nll_loss=1.01, nll_loss_teacher=0.539, kd_loss=0.409, ppl=2.01, wps=33474.8, ups=0.71, wpb=47241.5, bsz=2990.9, num_updates=224400, lr=6.67557e-05, gnorm=0.182, clip=0, loss_scale=8, train_wall=96, gb_free=18.9, wall=592
2022-11-02 05:36:03 | INFO | train_inner | epoch 015:    158 / 16028 loss=1.609, nll_loss=1.015, nll_loss_teacher=0.543, kd_loss=0.394, ppl=2.02, wps=32641.6, ups=0.73, wpb=45004.7, bsz=2986.1, num_updates=224500, lr=6.67409e-05, gnorm=0.192, clip=0, loss_scale=8, train_wall=138, gb_free=21.1, wall=729
2022-11-02 05:38:22 | INFO | train_inner | epoch 015:    258 / 16028 loss=1.587, nll_loss=1.007, nll_loss_teacher=0.54, kd_loss=0.356, ppl=2.01, wps=33255.3, ups=0.72, wpb=46226, bsz=3185, num_updates=224600, lr=6.6726e-05, gnorm=0.179, clip=0, loss_scale=8, train_wall=139, gb_free=19.6, wall=868
2022-11-02 05:40:40 | INFO | train_inner | epoch 015:    358 / 16028 loss=1.601, nll_loss=1.008, nll_loss_teacher=0.542, kd_loss=0.383, ppl=2.01, wps=33113.6, ups=0.72, wpb=45749.9, bsz=2991.9, num_updates=224700, lr=6.67112e-05, gnorm=0.181, clip=0, loss_scale=8, train_wall=138, gb_free=23.3, wall=1007
2022-11-02 05:42:58 | INFO | train_inner | epoch 015:    458 / 16028 loss=1.588, nll_loss=1.007, nll_loss_teacher=0.539, kd_loss=0.357, ppl=2.01, wps=33419.4, ups=0.73, wpb=46019.8, bsz=3060.2, num_updates=224800, lr=6.66963e-05, gnorm=0.18, clip=0, loss_scale=8, train_wall=138, gb_free=23, wall=1144
2022-11-02 05:45:15 | INFO | train_inner | epoch 015:    558 / 16028 loss=1.58, nll_loss=1, nll_loss_teacher=0.538, kd_loss=0.35, ppl=2, wps=33330.7, ups=0.73, wpb=45810.7, bsz=3362.7, num_updates=224900, lr=6.66815e-05, gnorm=0.177, clip=0, loss_scale=8, train_wall=137, gb_free=20.8, wall=1282
2022-11-02 05:47:33 | INFO | train_inner | epoch 015:    658 / 16028 loss=1.586, nll_loss=1, nll_loss_teacher=0.544, kd_loss=0.359, ppl=2, wps=33818.3, ups=0.73, wpb=46374.5, bsz=2944.1, num_updates=225000, lr=6.66667e-05, gnorm=0.183, clip=0, loss_scale=8, train_wall=137, gb_free=20, wall=1419
2022-11-02 05:49:51 | INFO | train_inner | epoch 015:    758 / 16028 loss=1.595, nll_loss=1.018, nll_loss_teacher=0.543, kd_loss=0.361, ppl=2.03, wps=32724.9, ups=0.72, wpb=45233.3, bsz=3024, num_updates=225100, lr=6.66519e-05, gnorm=0.185, clip=0, loss_scale=8, train_wall=138, gb_free=24.2, wall=1557
2022-11-02 05:52:07 | INFO | train_inner | epoch 015:    858 / 16028 loss=1.581, nll_loss=1.006, nll_loss_teacher=0.542, kd_loss=0.346, ppl=2.01, wps=33955.4, ups=0.74, wpb=46092.9, bsz=3166.8, num_updates=225200, lr=6.66371e-05, gnorm=0.179, clip=0, loss_scale=8, train_wall=136, gb_free=24.2, wall=1693
2022-11-02 05:54:23 | INFO | train_inner | epoch 015:    958 / 16028 loss=1.607, nll_loss=1.015, nll_loss_teacher=0.542, kd_loss=0.388, ppl=2.02, wps=32773.6, ups=0.74, wpb=44562.7, bsz=2966.1, num_updates=225300, lr=6.66223e-05, gnorm=0.192, clip=0, loss_scale=8, train_wall=136, gb_free=23.8, wall=1829
2022-11-02 05:56:39 | INFO | train_inner | epoch 015:   1058 / 16028 loss=1.576, nll_loss=1.012, nll_loss_teacher=0.545, kd_loss=0.328, ppl=2.02, wps=33644.2, ups=0.73, wpb=45811, bsz=3051.6, num_updates=225400, lr=6.66075e-05, gnorm=0.182, clip=0, loss_scale=8, train_wall=136, gb_free=21.3, wall=1965
2022-11-02 05:58:53 | INFO | train_inner | epoch 015:   1158 / 16028 loss=1.577, nll_loss=0.999, nll_loss_teacher=0.543, kd_loss=0.344, ppl=2, wps=33835.5, ups=0.74, wpb=45521.8, bsz=3308.4, num_updates=225500, lr=6.65927e-05, gnorm=0.18, clip=0, loss_scale=8, train_wall=134, gb_free=21.1, wall=2100
2022-11-02 06:01:08 | INFO | train_inner | epoch 015:   1258 / 16028 loss=1.593, nll_loss=1.013, nll_loss_teacher=0.545, kd_loss=0.363, ppl=2.02, wps=33131.4, ups=0.74, wpb=44632.4, bsz=3236.5, num_updates=225600, lr=6.6578e-05, gnorm=0.189, clip=0, loss_scale=8, train_wall=135, gb_free=24.9, wall=2234
2022-11-02 06:03:23 | INFO | train_inner | epoch 015:   1358 / 16028 loss=1.608, nll_loss=1.014, nll_loss_teacher=0.54, kd_loss=0.391, ppl=2.02, wps=33120.7, ups=0.74, wpb=44725, bsz=2986.7, num_updates=225700, lr=6.65632e-05, gnorm=0.191, clip=0, loss_scale=8, train_wall=135, gb_free=22.9, wall=2369
2022-11-02 06:05:40 | INFO | train_inner | epoch 015:   1458 / 16028 loss=1.587, nll_loss=1.008, nll_loss_teacher=0.545, kd_loss=0.354, ppl=2.01, wps=32920.5, ups=0.73, wpb=45018.6, bsz=3079.8, num_updates=225800, lr=6.65485e-05, gnorm=0.183, clip=0, loss_scale=8, train_wall=137, gb_free=21, wall=2506
2022-11-02 06:07:58 | INFO | train_inner | epoch 015:   1558 / 16028 loss=1.632, nll_loss=1.02, nll_loss_teacher=0.542, kd_loss=0.434, ppl=2.03, wps=32678.5, ups=0.73, wpb=45010.7, bsz=3009.9, num_updates=225900, lr=6.65337e-05, gnorm=0.196, clip=0, loss_scale=8, train_wall=138, gb_free=20.6, wall=2644
2022-11-02 06:10:14 | INFO | train_inner | epoch 015:   1658 / 16028 loss=1.609, nll_loss=1.007, nll_loss_teacher=0.541, kd_loss=0.401, ppl=2.01, wps=33576.7, ups=0.73, wpb=45785.9, bsz=3239, num_updates=226000, lr=6.6519e-05, gnorm=0.184, clip=0, loss_scale=8, train_wall=136, gb_free=18.7, wall=2780
2022-11-02 06:12:30 | INFO | train_inner | epoch 015:   1758 / 16028 loss=1.623, nll_loss=1.013, nll_loss_teacher=0.543, kd_loss=0.424, ppl=2.02, wps=33838.6, ups=0.73, wpb=46097.5, bsz=3076.6, num_updates=226100, lr=6.65043e-05, gnorm=0.186, clip=0, loss_scale=8, train_wall=136, gb_free=24.1, wall=2916
2022-11-02 06:14:46 | INFO | train_inner | epoch 015:   1858 / 16028 loss=1.597, nll_loss=1.011, nll_loss_teacher=0.539, kd_loss=0.373, ppl=2.02, wps=33566.8, ups=0.74, wpb=45460.8, bsz=3053, num_updates=226200, lr=6.64896e-05, gnorm=0.182, clip=0, loss_scale=8, train_wall=135, gb_free=23.5, wall=3052
2022-11-02 06:17:04 | INFO | train_inner | epoch 015:   1958 / 16028 loss=1.594, nll_loss=1.007, nll_loss_teacher=0.548, kd_loss=0.372, ppl=2.01, wps=33073.4, ups=0.72, wpb=45907.6, bsz=3065, num_updates=226300, lr=6.64749e-05, gnorm=0.179, clip=0, loss_scale=8, train_wall=139, gb_free=28.9, wall=3191
2022-11-02 06:19:19 | INFO | train_inner | epoch 015:   2058 / 16028 loss=1.597, nll_loss=1.002, nll_loss_teacher=0.541, kd_loss=0.382, ppl=2, wps=34513.1, ups=0.74, wpb=46355.6, bsz=3158.9, num_updates=226400, lr=6.64602e-05, gnorm=0.18, clip=0, loss_scale=8, train_wall=134, gb_free=22.9, wall=3325
2022-11-02 06:21:38 | INFO | train_inner | epoch 015:   2158 / 16028 loss=1.611, nll_loss=1.009, nll_loss_teacher=0.543, kd_loss=0.404, ppl=2.01, wps=32840.3, ups=0.72, wpb=45776.8, bsz=3207.7, num_updates=226500, lr=6.64455e-05, gnorm=0.185, clip=0, loss_scale=8, train_wall=139, gb_free=17.6, wall=3464
2022-11-02 06:23:55 | INFO | train_inner | epoch 015:   2258 / 16028 loss=1.581, nll_loss=1.008, nll_loss_teacher=0.541, kd_loss=0.344, ppl=2.01, wps=33628.1, ups=0.73, wpb=45887, bsz=3180.8, num_updates=226600, lr=6.64309e-05, gnorm=0.179, clip=0, loss_scale=8, train_wall=136, gb_free=21.6, wall=3601
2022-11-02 06:26:13 | INFO | train_inner | epoch 015:   2358 / 16028 loss=1.619, nll_loss=1.006, nll_loss_teacher=0.542, kd_loss=0.423, ppl=2.01, wps=33021, ups=0.72, wpb=45874, bsz=2990.8, num_updates=226700, lr=6.64162e-05, gnorm=0.19, clip=0, loss_scale=8, train_wall=139, gb_free=24.6, wall=3740
2022-11-02 06:28:29 | INFO | train_inner | epoch 015:   2458 / 16028 loss=1.598, nll_loss=1.006, nll_loss_teacher=0.542, kd_loss=0.38, ppl=2.01, wps=33664.1, ups=0.74, wpb=45791.6, bsz=3206.5, num_updates=226800, lr=6.64016e-05, gnorm=0.181, clip=0, loss_scale=8, train_wall=136, gb_free=25.6, wall=3876
2022-11-02 06:30:47 | INFO | train_inner | epoch 015:   2558 / 16028 loss=1.584, nll_loss=1.014, nll_loss_teacher=0.538, kd_loss=0.345, ppl=2.02, wps=33301.2, ups=0.73, wpb=45827.8, bsz=3065.3, num_updates=226900, lr=6.6387e-05, gnorm=0.184, clip=0, loss_scale=8, train_wall=137, gb_free=26.2, wall=4013
2022-11-02 06:33:00 | INFO | train_inner | epoch 015:   2658 / 16028 loss=1.602, nll_loss=1.011, nll_loss_teacher=0.54, kd_loss=0.383, ppl=2.01, wps=34923.1, ups=0.75, wpb=46449.4, bsz=3067.8, num_updates=227000, lr=6.63723e-05, gnorm=0.185, clip=0, loss_scale=8, train_wall=133, gb_free=20.6, wall=4146
2022-11-02 06:35:14 | INFO | train_inner | epoch 015:   2758 / 16028 loss=1.595, nll_loss=1.01, nll_loss_teacher=0.543, kd_loss=0.37, ppl=2.01, wps=33905.9, ups=0.74, wpb=45511.7, bsz=3097.6, num_updates=227100, lr=6.63577e-05, gnorm=0.183, clip=0, loss_scale=8, train_wall=134, gb_free=24.2, wall=4281
2022-11-02 06:37:31 | INFO | train_inner | epoch 015:   2858 / 16028 loss=1.6, nll_loss=1.007, nll_loss_teacher=0.544, kd_loss=0.383, ppl=2.01, wps=33554.3, ups=0.73, wpb=45749.9, bsz=3176.9, num_updates=227200, lr=6.63431e-05, gnorm=0.181, clip=0, loss_scale=8, train_wall=136, gb_free=19.1, wall=4417
2022-11-02 06:39:48 | INFO | train_inner | epoch 015:   2958 / 16028 loss=1.606, nll_loss=1.024, nll_loss_teacher=0.544, kd_loss=0.379, ppl=2.03, wps=33570.4, ups=0.73, wpb=45952.3, bsz=3159.9, num_updates=227300, lr=6.63285e-05, gnorm=0.195, clip=0, loss_scale=8, train_wall=137, gb_free=22.5, wall=4554
2022-11-02 06:42:04 | INFO | train_inner | epoch 015:   3058 / 16028 loss=1.6, nll_loss=1.013, nll_loss_teacher=0.545, kd_loss=0.377, ppl=2.02, wps=34000.5, ups=0.74, wpb=46254.1, bsz=3093.8, num_updates=227400, lr=6.63139e-05, gnorm=0.181, clip=0, loss_scale=8, train_wall=136, gb_free=22.9, wall=4690
2022-11-02 06:44:19 | INFO | train_inner | epoch 015:   3158 / 16028 loss=1.591, nll_loss=1.007, nll_loss_teacher=0.546, kd_loss=0.364, ppl=2.01, wps=33902.9, ups=0.74, wpb=45916, bsz=3136.6, num_updates=227500, lr=6.62994e-05, gnorm=0.181, clip=0, loss_scale=8, train_wall=135, gb_free=22.3, wall=4825
2022-11-02 06:46:38 | INFO | train_inner | epoch 015:   3258 / 16028 loss=1.593, nll_loss=1.01, nll_loss_teacher=0.541, kd_loss=0.366, ppl=2.01, wps=33338.5, ups=0.72, wpb=46223.4, bsz=2983.1, num_updates=227600, lr=6.62848e-05, gnorm=0.176, clip=0, loss_scale=8, train_wall=138, gb_free=21.4, wall=4964
2022-11-02 06:48:55 | INFO | train_inner | epoch 015:   3358 / 16028 loss=1.596, nll_loss=1.012, nll_loss_teacher=0.543, kd_loss=0.371, ppl=2.02, wps=33674.4, ups=0.73, wpb=46127.7, bsz=3146.4, num_updates=227700, lr=6.62702e-05, gnorm=0.18, clip=0, loss_scale=8, train_wall=137, gb_free=20.2, wall=5101
2022-11-02 06:51:11 | INFO | train_inner | epoch 015:   3458 / 16028 loss=1.63, nll_loss=1.017, nll_loss_teacher=0.543, kd_loss=0.433, ppl=2.02, wps=33679.7, ups=0.74, wpb=45809.1, bsz=2971.8, num_updates=227800, lr=6.62557e-05, gnorm=0.183, clip=0, loss_scale=8, train_wall=136, gb_free=19.9, wall=5237
2022-11-02 06:53:26 | INFO | train_inner | epoch 015:   3558 / 16028 loss=1.606, nll_loss=1.014, nll_loss_teacher=0.542, kd_loss=0.389, ppl=2.02, wps=33987.6, ups=0.74, wpb=45956.8, bsz=3168.4, num_updates=227900, lr=6.62411e-05, gnorm=0.186, clip=0, loss_scale=8, train_wall=135, gb_free=16.7, wall=5372
2022-11-02 06:55:43 | INFO | train_inner | epoch 015:   3658 / 16028 loss=1.614, nll_loss=1.013, nll_loss_teacher=0.542, kd_loss=0.405, ppl=2.02, wps=33156.9, ups=0.73, wpb=45554.9, bsz=2920.2, num_updates=228000, lr=6.62266e-05, gnorm=0.186, clip=0, loss_scale=8, train_wall=137, gb_free=25.3, wall=5510
2022-11-02 06:58:01 | INFO | train_inner | epoch 015:   3758 / 16028 loss=1.617, nll_loss=1.007, nll_loss_teacher=0.54, kd_loss=0.417, ppl=2.01, wps=32967.7, ups=0.73, wpb=45373.8, bsz=3124.2, num_updates=228100, lr=6.62121e-05, gnorm=0.187, clip=0, loss_scale=8, train_wall=137, gb_free=21.3, wall=5647
2022-11-02 07:00:19 | INFO | train_inner | epoch 015:   3858 / 16028 loss=1.613, nll_loss=1.018, nll_loss_teacher=0.544, kd_loss=0.399, ppl=2.02, wps=33349, ups=0.73, wpb=45902.5, bsz=2993, num_updates=228200, lr=6.61976e-05, gnorm=0.185, clip=0, loss_scale=8, train_wall=137, gb_free=20.7, wall=5785
2022-11-02 07:02:35 | INFO | train_inner | epoch 015:   3958 / 16028 loss=1.597, nll_loss=1.011, nll_loss_teacher=0.542, kd_loss=0.373, ppl=2.01, wps=33524.3, ups=0.73, wpb=45751, bsz=3039.9, num_updates=228300, lr=6.61831e-05, gnorm=0.184, clip=0, loss_scale=8, train_wall=136, gb_free=23.4, wall=5921
2022-11-02 07:04:54 | INFO | train_inner | epoch 015:   4058 / 16028 loss=1.58, nll_loss=1.009, nll_loss_teacher=0.543, kd_loss=0.342, ppl=2.01, wps=32290.3, ups=0.72, wpb=44985.9, bsz=3130.6, num_updates=228400, lr=6.61686e-05, gnorm=0.184, clip=0, loss_scale=8, train_wall=139, gb_free=27.7, wall=6061
2022-11-02 07:07:11 | INFO | train_inner | epoch 015:   4158 / 16028 loss=1.598, nll_loss=1.006, nll_loss_teacher=0.541, kd_loss=0.378, ppl=2.01, wps=33549.5, ups=0.73, wpb=45892.5, bsz=2934.1, num_updates=228500, lr=6.61541e-05, gnorm=0.182, clip=0, loss_scale=16, train_wall=137, gb_free=24.5, wall=6198
2022-11-02 07:09:30 | INFO | train_inner | epoch 015:   4258 / 16028 loss=1.599, nll_loss=1.009, nll_loss_teacher=0.541, kd_loss=0.379, ppl=2.01, wps=33074.2, ups=0.72, wpb=45792.4, bsz=3012.2, num_updates=228600, lr=6.61396e-05, gnorm=0.184, clip=0, loss_scale=16, train_wall=138, gb_free=24.7, wall=6336
2022-11-02 07:11:48 | INFO | train_inner | epoch 015:   4358 / 16028 loss=1.585, nll_loss=0.999, nll_loss_teacher=0.542, kd_loss=0.362, ppl=2, wps=33117.6, ups=0.72, wpb=45934.1, bsz=3366.1, num_updates=228700, lr=6.61252e-05, gnorm=0.175, clip=0, loss_scale=16, train_wall=139, gb_free=22.6, wall=6475
2022-11-02 07:14:05 | INFO | train_inner | epoch 015:   4458 / 16028 loss=1.596, nll_loss=1.022, nll_loss_teacher=0.541, kd_loss=0.361, ppl=2.03, wps=33849, ups=0.73, wpb=46328.2, bsz=3205.4, num_updates=228800, lr=6.61107e-05, gnorm=0.195, clip=1, loss_scale=16, train_wall=137, gb_free=17, wall=6612
2022-11-02 07:16:22 | INFO | train_inner | epoch 015:   4558 / 16028 loss=1.597, nll_loss=1.007, nll_loss_teacher=0.542, kd_loss=0.378, ppl=2.01, wps=34102.1, ups=0.73, wpb=46531.6, bsz=3140.8, num_updates=228900, lr=6.60963e-05, gnorm=0.178, clip=0, loss_scale=16, train_wall=136, gb_free=20.6, wall=6748
2022-11-02 07:17:05 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-11-02 07:18:41 | INFO | train_inner | epoch 015:   4659 / 16028 loss=1.629, nll_loss=1.014, nll_loss_teacher=0.543, kd_loss=0.434, ppl=2.02, wps=33533.4, ups=0.72, wpb=46836, bsz=2992.2, num_updates=229000, lr=6.60819e-05, gnorm=0.184, clip=0, loss_scale=8, train_wall=140, gb_free=17, wall=6888
2022-11-02 07:20:58 | INFO | train_inner | epoch 015:   4759 / 16028 loss=1.608, nll_loss=1.009, nll_loss_teacher=0.543, kd_loss=0.396, ppl=2.01, wps=33784.6, ups=0.73, wpb=46129.9, bsz=3049.2, num_updates=229100, lr=6.60674e-05, gnorm=0.19, clip=0, loss_scale=8, train_wall=136, gb_free=26.5, wall=7024
2022-11-02 07:23:16 | INFO | train_inner | epoch 015:   4859 / 16028 loss=1.586, nll_loss=1.007, nll_loss_teacher=0.543, kd_loss=0.354, ppl=2.01, wps=33207.2, ups=0.73, wpb=45697.9, bsz=2937.8, num_updates=229200, lr=6.6053e-05, gnorm=0.18, clip=0, loss_scale=8, train_wall=137, gb_free=22.3, wall=7162
2022-11-02 07:25:31 | INFO | train_inner | epoch 015:   4959 / 16028 loss=1.618, nll_loss=1.002, nll_loss_teacher=0.543, kd_loss=0.425, ppl=2, wps=33414.5, ups=0.74, wpb=45213.2, bsz=3303, num_updates=229300, lr=6.60386e-05, gnorm=0.189, clip=0, loss_scale=8, train_wall=135, gb_free=21.1, wall=7297
2022-11-02 07:27:45 | INFO | train_inner | epoch 015:   5059 / 16028 loss=1.6, nll_loss=1.007, nll_loss_teacher=0.542, kd_loss=0.383, ppl=2.01, wps=33774.5, ups=0.75, wpb=45265.1, bsz=3129.7, num_updates=229400, lr=6.60242e-05, gnorm=0.179, clip=0, loss_scale=8, train_wall=134, gb_free=18, wall=7431
2022-11-02 07:30:00 | INFO | train_inner | epoch 015:   5159 / 16028 loss=1.629, nll_loss=1.017, nll_loss_teacher=0.544, kd_loss=0.432, ppl=2.02, wps=34212.8, ups=0.74, wpb=46117.6, bsz=2965.5, num_updates=229500, lr=6.60098e-05, gnorm=0.187, clip=0, loss_scale=8, train_wall=135, gb_free=22.2, wall=7566
2022-11-02 07:32:15 | INFO | train_inner | epoch 015:   5259 / 16028 loss=1.607, nll_loss=1.02, nll_loss_teacher=0.546, kd_loss=0.384, ppl=2.03, wps=33586, ups=0.74, wpb=45397.6, bsz=2955.8, num_updates=229600, lr=6.59955e-05, gnorm=0.188, clip=0, loss_scale=8, train_wall=135, gb_free=16.9, wall=7701
2022-11-02 07:34:32 | INFO | train_inner | epoch 015:   5359 / 16028 loss=1.608, nll_loss=1.001, nll_loss_teacher=0.542, kd_loss=0.405, ppl=2, wps=33590.3, ups=0.73, wpb=46081.6, bsz=3175.8, num_updates=229700, lr=6.59811e-05, gnorm=0.18, clip=0, loss_scale=8, train_wall=137, gb_free=20.2, wall=7838
2022-11-02 07:36:50 | INFO | train_inner | epoch 015:   5459 / 16028 loss=1.61, nll_loss=1.012, nll_loss_teacher=0.539, kd_loss=0.398, ppl=2.02, wps=33525.9, ups=0.73, wpb=46187.3, bsz=3123.5, num_updates=229800, lr=6.59667e-05, gnorm=0.183, clip=0, loss_scale=8, train_wall=138, gb_free=16.6, wall=7976
2022-11-02 07:39:08 | INFO | train_inner | epoch 015:   5559 / 16028 loss=1.602, nll_loss=1.003, nll_loss_teacher=0.54, kd_loss=0.39, ppl=2, wps=33418.4, ups=0.72, wpb=46257.4, bsz=3139.8, num_updates=229900, lr=6.59524e-05, gnorm=0.177, clip=0, loss_scale=8, train_wall=138, gb_free=22.2, wall=8115
2022-11-02 07:41:25 | INFO | train_inner | epoch 015:   5659 / 16028 loss=1.624, nll_loss=1.012, nll_loss_teacher=0.544, kd_loss=0.427, ppl=2.02, wps=33661.8, ups=0.73, wpb=46120.5, bsz=3043.9, num_updates=230000, lr=6.5938e-05, gnorm=0.189, clip=0, loss_scale=8, train_wall=137, gb_free=22.3, wall=8252
2022-11-02 07:43:42 | INFO | train_inner | epoch 015:   5759 / 16028 loss=1.592, nll_loss=1.011, nll_loss_teacher=0.541, kd_loss=0.363, ppl=2.02, wps=33596.5, ups=0.73, wpb=46048.7, bsz=3027, num_updates=230100, lr=6.59237e-05, gnorm=0.185, clip=0, loss_scale=8, train_wall=137, gb_free=20.2, wall=8389
2022-11-02 07:46:01 | INFO | train_inner | epoch 015:   5859 / 16028 loss=1.612, nll_loss=1.013, nll_loss_teacher=0.544, kd_loss=0.402, ppl=2.02, wps=33135.2, ups=0.72, wpb=45904.3, bsz=3126.2, num_updates=230200, lr=6.59094e-05, gnorm=0.189, clip=0, loss_scale=8, train_wall=138, gb_free=21.4, wall=8527
2022-11-02 07:48:19 | INFO | train_inner | epoch 015:   5959 / 16028 loss=1.602, nll_loss=1.013, nll_loss_teacher=0.54, kd_loss=0.381, ppl=2.02, wps=33116.2, ups=0.72, wpb=45786.6, bsz=3096.1, num_updates=230300, lr=6.58951e-05, gnorm=0.187, clip=0, loss_scale=8, train_wall=138, gb_free=21.7, wall=8665
2022-11-02 07:50:33 | INFO | train_inner | epoch 015:   6059 / 16028 loss=1.586, nll_loss=1.003, nll_loss_teacher=0.541, kd_loss=0.359, ppl=2, wps=33915.7, ups=0.75, wpb=45384.8, bsz=3272.2, num_updates=230400, lr=6.58808e-05, gnorm=0.18, clip=0, loss_scale=8, train_wall=134, gb_free=25.4, wall=8799
2022-11-02 07:52:53 | INFO | train_inner | epoch 015:   6159 / 16028 loss=1.6, nll_loss=1.016, nll_loss_teacher=0.547, kd_loss=0.375, ppl=2.02, wps=32485.8, ups=0.72, wpb=45394.3, bsz=3099.2, num_updates=230500, lr=6.58665e-05, gnorm=0.19, clip=0, loss_scale=8, train_wall=140, gb_free=23.2, wall=8939
2022-11-02 07:55:08 | INFO | train_inner | epoch 015:   6259 / 16028 loss=1.598, nll_loss=1.018, nll_loss_teacher=0.543, kd_loss=0.368, ppl=2.03, wps=33642.8, ups=0.74, wpb=45554.4, bsz=3168.2, num_updates=230600, lr=6.58522e-05, gnorm=0.187, clip=0, loss_scale=8, train_wall=135, gb_free=27.1, wall=9074
2022-11-02 07:57:26 | INFO | train_inner | epoch 015:   6359 / 16028 loss=1.6, nll_loss=1.011, nll_loss_teacher=0.542, kd_loss=0.378, ppl=2.02, wps=33721.3, ups=0.72, wpb=46670.1, bsz=3075.6, num_updates=230700, lr=6.58379e-05, gnorm=0.182, clip=0, loss_scale=8, train_wall=138, gb_free=19.6, wall=9213
2022-11-02 07:59:43 | INFO | train_inner | epoch 015:   6459 / 16028 loss=1.595, nll_loss=1, nll_loss_teacher=0.54, kd_loss=0.381, ppl=2, wps=33957.8, ups=0.73, wpb=46271.1, bsz=3135.7, num_updates=230800, lr=6.58237e-05, gnorm=0.179, clip=0, loss_scale=8, train_wall=136, gb_free=20, wall=9349
2022-11-02 08:02:01 | INFO | train_inner | epoch 015:   6559 / 16028 loss=1.589, nll_loss=1.006, nll_loss_teacher=0.542, kd_loss=0.362, ppl=2.01, wps=33074, ups=0.72, wpb=45639.7, bsz=3033.3, num_updates=230900, lr=6.58094e-05, gnorm=0.183, clip=0, loss_scale=8, train_wall=138, gb_free=22.8, wall=9487
2022-11-02 08:04:18 | INFO | train_inner | epoch 015:   6659 / 16028 loss=1.605, nll_loss=1.006, nll_loss_teacher=0.544, kd_loss=0.395, ppl=2.01, wps=33714.6, ups=0.73, wpb=46323.2, bsz=3294.3, num_updates=231000, lr=6.57952e-05, gnorm=0.184, clip=0, loss_scale=8, train_wall=137, gb_free=24.6, wall=9624
2022-11-02 08:06:33 | INFO | train_inner | epoch 015:   6759 / 16028 loss=1.602, nll_loss=1.009, nll_loss_teacher=0.541, kd_loss=0.384, ppl=2.01, wps=34123.8, ups=0.74, wpb=46135.5, bsz=3004.2, num_updates=231100, lr=6.57809e-05, gnorm=0.181, clip=0, loss_scale=8, train_wall=135, gb_free=23, wall=9760
2022-11-02 08:08:52 | INFO | train_inner | epoch 015:   6859 / 16028 loss=1.605, nll_loss=1.01, nll_loss_teacher=0.546, kd_loss=0.39, ppl=2.01, wps=32718.8, ups=0.72, wpb=45367.3, bsz=3215.9, num_updates=231200, lr=6.57667e-05, gnorm=0.188, clip=0, loss_scale=8, train_wall=138, gb_free=27.3, wall=9898
2022-11-02 08:11:08 | INFO | train_inner | epoch 015:   6959 / 16028 loss=1.598, nll_loss=1.013, nll_loss_teacher=0.544, kd_loss=0.375, ppl=2.02, wps=33586.7, ups=0.73, wpb=45791.5, bsz=3087.2, num_updates=231300, lr=6.57525e-05, gnorm=0.188, clip=0, loss_scale=8, train_wall=136, gb_free=20.5, wall=10035
2022-11-02 08:13:26 | INFO | train_inner | epoch 015:   7059 / 16028 loss=1.606, nll_loss=1.009, nll_loss_teacher=0.544, kd_loss=0.394, ppl=2.01, wps=33273, ups=0.73, wpb=45866.2, bsz=3064.5, num_updates=231400, lr=6.57383e-05, gnorm=0.181, clip=0, loss_scale=8, train_wall=138, gb_free=19, wall=10172
2022-11-02 08:15:43 | INFO | train_inner | epoch 015:   7159 / 16028 loss=1.615, nll_loss=1.013, nll_loss_teacher=0.54, kd_loss=0.409, ppl=2.02, wps=33591.3, ups=0.73, wpb=46062.2, bsz=3121.1, num_updates=231500, lr=6.57241e-05, gnorm=0.189, clip=0, loss_scale=8, train_wall=137, gb_free=19.7, wall=10310
2022-11-02 08:17:55 | INFO | train_inner | epoch 015:   7259 / 16028 loss=1.604, nll_loss=1.005, nll_loss_teacher=0.542, kd_loss=0.393, ppl=2.01, wps=35235.7, ups=0.76, wpb=46552.5, bsz=3039.2, num_updates=231600, lr=6.57099e-05, gnorm=0.179, clip=0, loss_scale=8, train_wall=132, gb_free=22.7, wall=10442
2022-11-02 08:20:04 | INFO | train_inner | epoch 015:   7359 / 16028 loss=1.6, nll_loss=1.019, nll_loss_teacher=0.543, kd_loss=0.371, ppl=2.03, wps=35629.9, ups=0.78, wpb=45780.8, bsz=2929.3, num_updates=231700, lr=6.56957e-05, gnorm=0.185, clip=0, loss_scale=8, train_wall=128, gb_free=17.6, wall=10570
2022-11-02 08:22:12 | INFO | train_inner | epoch 015:   7459 / 16028 loss=1.607, nll_loss=1.007, nll_loss_teacher=0.54, kd_loss=0.397, ppl=2.01, wps=35784.8, ups=0.78, wpb=45692.6, bsz=3118.4, num_updates=231800, lr=6.56815e-05, gnorm=0.193, clip=0, loss_scale=8, train_wall=127, gb_free=16.6, wall=10698
2022-11-02 08:24:19 | INFO | train_inner | epoch 015:   7559 / 16028 loss=1.609, nll_loss=1.009, nll_loss_teacher=0.54, kd_loss=0.4, ppl=2.01, wps=36724.2, ups=0.78, wpb=46888.2, bsz=3125.4, num_updates=231900, lr=6.56674e-05, gnorm=0.184, clip=0, loss_scale=8, train_wall=127, gb_free=24.3, wall=10826
2022-11-02 08:26:27 | INFO | train_inner | epoch 015:   7659 / 16028 loss=1.617, nll_loss=1.009, nll_loss_teacher=0.546, kd_loss=0.416, ppl=2.01, wps=36225.7, ups=0.79, wpb=46123.1, bsz=2963.9, num_updates=232000, lr=6.56532e-05, gnorm=0.183, clip=0, loss_scale=8, train_wall=127, gb_free=22.6, wall=10953
2022-11-02 08:28:36 | INFO | train_inner | epoch 015:   7759 / 16028 loss=1.592, nll_loss=1, nll_loss_teacher=0.543, kd_loss=0.375, ppl=2, wps=35679.2, ups=0.77, wpb=46226.3, bsz=3323.4, num_updates=232100, lr=6.56391e-05, gnorm=0.182, clip=0, loss_scale=8, train_wall=129, gb_free=18.1, wall=11082
2022-11-02 08:30:51 | INFO | train_inner | epoch 015:   7859 / 16028 loss=1.592, nll_loss=1.011, nll_loss_teacher=0.541, kd_loss=0.364, ppl=2.01, wps=34534.1, ups=0.74, wpb=46505.4, bsz=3105.9, num_updates=232200, lr=6.56249e-05, gnorm=0.182, clip=0, loss_scale=8, train_wall=134, gb_free=19, wall=11217
2022-11-02 08:33:05 | INFO | train_inner | epoch 015:   7959 / 16028 loss=1.597, nll_loss=1.013, nll_loss_teacher=0.547, kd_loss=0.372, ppl=2.02, wps=34149.3, ups=0.75, wpb=45722.3, bsz=3130.3, num_updates=232300, lr=6.56108e-05, gnorm=0.186, clip=0, loss_scale=8, train_wall=134, gb_free=22.4, wall=11351
2022-11-02 08:35:21 | INFO | train_inner | epoch 015:   8059 / 16028 loss=1.614, nll_loss=1.019, nll_loss_teacher=0.546, kd_loss=0.4, ppl=2.03, wps=33508.5, ups=0.73, wpb=45599.1, bsz=3097.6, num_updates=232400, lr=6.55967e-05, gnorm=0.192, clip=0, loss_scale=8, train_wall=136, gb_free=28.2, wall=11487
2022-11-02 08:37:39 | INFO | train_inner | epoch 015:   8159 / 16028 loss=1.613, nll_loss=1.02, nll_loss_teacher=0.542, kd_loss=0.397, ppl=2.03, wps=32950.9, ups=0.73, wpb=45444.5, bsz=3059.1, num_updates=232500, lr=6.55826e-05, gnorm=0.189, clip=0, loss_scale=8, train_wall=138, gb_free=21.7, wall=11625
2022-11-02 08:39:55 | INFO | train_inner | epoch 015:   8259 / 16028 loss=1.603, nll_loss=1.019, nll_loss_teacher=0.545, kd_loss=0.378, ppl=2.03, wps=33625.3, ups=0.73, wpb=45969.1, bsz=2955.4, num_updates=232600, lr=6.55685e-05, gnorm=0.184, clip=0, loss_scale=8, train_wall=137, gb_free=20.5, wall=11762
2022-11-02 08:42:14 | INFO | train_inner | epoch 015:   8359 / 16028 loss=1.602, nll_loss=1.01, nll_loss_teacher=0.542, kd_loss=0.386, ppl=2.01, wps=32945.4, ups=0.72, wpb=45633.5, bsz=3238, num_updates=232700, lr=6.55544e-05, gnorm=0.184, clip=0, loss_scale=8, train_wall=138, gb_free=27.5, wall=11900
2022-11-02 08:44:33 | INFO | train_inner | epoch 015:   8459 / 16028 loss=1.612, nll_loss=1.009, nll_loss_teacher=0.545, kd_loss=0.405, ppl=2.01, wps=33771.4, ups=0.72, wpb=46878.4, bsz=3076.7, num_updates=232800, lr=6.55403e-05, gnorm=0.184, clip=0, loss_scale=8, train_wall=139, gb_free=16.8, wall=12039
2022-11-02 08:46:50 | INFO | train_inner | epoch 015:   8559 / 16028 loss=1.599, nll_loss=1.012, nll_loss_teacher=0.541, kd_loss=0.378, ppl=2.02, wps=33504.3, ups=0.73, wpb=45978.4, bsz=3166.2, num_updates=232900, lr=6.55262e-05, gnorm=0.185, clip=0, loss_scale=8, train_wall=137, gb_free=21, wall=12176
2022-11-02 08:49:08 | INFO | train_inner | epoch 015:   8659 / 16028 loss=1.595, nll_loss=1.014, nll_loss_teacher=0.54, kd_loss=0.366, ppl=2.02, wps=32927.5, ups=0.72, wpb=45449.7, bsz=3113.7, num_updates=233000, lr=6.55122e-05, gnorm=0.191, clip=0, loss_scale=8, train_wall=138, gb_free=22.6, wall=12314
2022-11-02 08:51:22 | INFO | train_inner | epoch 015:   8759 / 16028 loss=1.601, nll_loss=1.01, nll_loss_teacher=0.546, kd_loss=0.382, ppl=2.01, wps=34392.9, ups=0.75, wpb=46057.1, bsz=3064.6, num_updates=233100, lr=6.54981e-05, gnorm=0.18, clip=0, loss_scale=16, train_wall=134, gb_free=21.2, wall=12448
2022-11-02 08:53:38 | INFO | train_inner | epoch 015:   8859 / 16028 loss=1.601, nll_loss=1.015, nll_loss_teacher=0.543, kd_loss=0.378, ppl=2.02, wps=33461.2, ups=0.74, wpb=45369.4, bsz=3056, num_updates=233200, lr=6.54841e-05, gnorm=0.185, clip=0, loss_scale=16, train_wall=135, gb_free=21.1, wall=12584
2022-11-02 08:55:54 | INFO | train_inner | epoch 015:   8959 / 16028 loss=1.596, nll_loss=1.017, nll_loss_teacher=0.542, kd_loss=0.365, ppl=2.02, wps=33832.6, ups=0.73, wpb=46077.2, bsz=3085.4, num_updates=233300, lr=6.547e-05, gnorm=0.187, clip=0, loss_scale=16, train_wall=136, gb_free=23.7, wall=12720
2022-11-02 08:58:09 | INFO | train_inner | epoch 015:   9059 / 16028 loss=1.614, nll_loss=1.015, nll_loss_teacher=0.545, kd_loss=0.404, ppl=2.02, wps=34653.9, ups=0.74, wpb=46972.7, bsz=3133.8, num_updates=233400, lr=6.5456e-05, gnorm=0.193, clip=0, loss_scale=16, train_wall=135, gb_free=23.3, wall=12856
2022-11-02 09:00:25 | INFO | train_inner | epoch 015:   9159 / 16028 loss=1.596, nll_loss=1.015, nll_loss_teacher=0.541, kd_loss=0.368, ppl=2.02, wps=34229.4, ups=0.74, wpb=46448.8, bsz=3140.3, num_updates=233500, lr=6.5442e-05, gnorm=0.187, clip=0, loss_scale=16, train_wall=136, gb_free=18.1, wall=12991
2022-11-02 09:02:41 | INFO | train_inner | epoch 015:   9259 / 16028 loss=1.608, nll_loss=1.011, nll_loss_teacher=0.539, kd_loss=0.394, ppl=2.02, wps=33471.4, ups=0.73, wpb=45547.4, bsz=3024.4, num_updates=233600, lr=6.5428e-05, gnorm=0.184, clip=0, loss_scale=16, train_wall=136, gb_free=16.6, wall=13127
2022-11-02 09:04:59 | INFO | train_inner | epoch 015:   9359 / 16028 loss=1.613, nll_loss=1.011, nll_loss_teacher=0.541, kd_loss=0.405, ppl=2.02, wps=32987.4, ups=0.73, wpb=45494.8, bsz=2981.5, num_updates=233700, lr=6.5414e-05, gnorm=0.186, clip=0, loss_scale=16, train_wall=138, gb_free=22, wall=13265
2022-11-02 09:07:17 | INFO | train_inner | epoch 015:   9459 / 16028 loss=1.619, nll_loss=1.011, nll_loss_teacher=0.545, kd_loss=0.418, ppl=2.02, wps=33275.8, ups=0.73, wpb=45771.2, bsz=3018.3, num_updates=233800, lr=6.54e-05, gnorm=0.188, clip=0, loss_scale=16, train_wall=137, gb_free=20.8, wall=13403
2022-11-02 09:09:33 | INFO | train_inner | epoch 015:   9559 / 16028 loss=1.595, nll_loss=1.004, nll_loss_teacher=0.544, kd_loss=0.377, ppl=2.01, wps=33745.8, ups=0.73, wpb=46006.1, bsz=3152.5, num_updates=233900, lr=6.5386e-05, gnorm=0.177, clip=0, loss_scale=16, train_wall=136, gb_free=21, wall=13539
2022-11-02 09:11:49 | INFO | train_inner | epoch 015:   9659 / 16028 loss=1.607, nll_loss=1.013, nll_loss_teacher=0.545, kd_loss=0.392, ppl=2.02, wps=33930.2, ups=0.74, wpb=46103.8, bsz=3187.6, num_updates=234000, lr=6.5372e-05, gnorm=0.181, clip=0, loss_scale=16, train_wall=136, gb_free=21.6, wall=13675
2022-11-02 09:14:05 | INFO | train_inner | epoch 015:   9759 / 16028 loss=1.61, nll_loss=1.018, nll_loss_teacher=0.542, kd_loss=0.394, ppl=2.03, wps=33859.9, ups=0.73, wpb=46129.4, bsz=3022.9, num_updates=234100, lr=6.53581e-05, gnorm=0.192, clip=0, loss_scale=16, train_wall=136, gb_free=22.4, wall=13811
2022-11-02 09:16:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-11-02 09:16:22 | INFO | train_inner | epoch 015:   9860 / 16028 loss=1.597, nll_loss=1.021, nll_loss_teacher=0.54, kd_loss=0.364, ppl=2.03, wps=33097.7, ups=0.73, wpb=45369.7, bsz=2996.4, num_updates=234200, lr=6.53441e-05, gnorm=0.19, clip=0, loss_scale=8, train_wall=137, gb_free=20.4, wall=13948
2022-11-02 09:18:40 | INFO | train_inner | epoch 015:   9960 / 16028 loss=1.614, nll_loss=1.013, nll_loss_teacher=0.542, kd_loss=0.406, ppl=2.02, wps=33149.7, ups=0.72, wpb=45854.9, bsz=3013.9, num_updates=234300, lr=6.53302e-05, gnorm=0.186, clip=0, loss_scale=8, train_wall=138, gb_free=20, wall=14087
2022-11-02 09:20:57 | INFO | train_inner | epoch 015:  10060 / 16028 loss=1.586, nll_loss=1.014, nll_loss_teacher=0.543, kd_loss=0.348, ppl=2.02, wps=34449.6, ups=0.73, wpb=47081.5, bsz=2973.2, num_updates=234400, lr=6.53162e-05, gnorm=0.181, clip=0, loss_scale=8, train_wall=136, gb_free=25.1, wall=14223
2022-11-02 09:23:14 | INFO | train_inner | epoch 015:  10160 / 16028 loss=1.58, nll_loss=1.009, nll_loss_teacher=0.541, kd_loss=0.34, ppl=2.01, wps=34001.6, ups=0.73, wpb=46526.5, bsz=2966.1, num_updates=234500, lr=6.53023e-05, gnorm=0.181, clip=0, loss_scale=8, train_wall=137, gb_free=18.7, wall=14360
2022-11-02 09:25:31 | INFO | train_inner | epoch 015:  10260 / 16028 loss=1.594, nll_loss=1.012, nll_loss_teacher=0.546, kd_loss=0.366, ppl=2.02, wps=33042.4, ups=0.73, wpb=45280.5, bsz=3219, num_updates=234600, lr=6.52884e-05, gnorm=0.188, clip=0, loss_scale=8, train_wall=137, gb_free=24.3, wall=14497
2022-11-02 09:27:51 | INFO | train_inner | epoch 015:  10360 / 16028 loss=1.613, nll_loss=1.009, nll_loss_teacher=0.546, kd_loss=0.409, ppl=2.01, wps=33007.5, ups=0.71, wpb=46331.7, bsz=3252.5, num_updates=234700, lr=6.52745e-05, gnorm=0.184, clip=0, loss_scale=8, train_wall=140, gb_free=19.8, wall=14638
2022-11-02 09:30:08 | INFO | train_inner | epoch 015:  10460 / 16028 loss=1.593, nll_loss=1.005, nll_loss_teacher=0.544, kd_loss=0.371, ppl=2.01, wps=33325.5, ups=0.73, wpb=45488.7, bsz=3208.6, num_updates=234800, lr=6.52606e-05, gnorm=0.185, clip=0, loss_scale=8, train_wall=136, gb_free=20.6, wall=14774
2022-11-02 09:32:26 | INFO | train_inner | epoch 015:  10560 / 16028 loss=1.588, nll_loss=1.019, nll_loss_teacher=0.543, kd_loss=0.348, ppl=2.03, wps=32346.4, ups=0.72, wpb=44770.9, bsz=3221.8, num_updates=234900, lr=6.52467e-05, gnorm=0.186, clip=0, loss_scale=8, train_wall=138, gb_free=17.1, wall=14913
2022-11-02 09:34:42 | INFO | train_inner | epoch 015:  10660 / 16028 loss=1.603, nll_loss=1.008, nll_loss_teacher=0.542, kd_loss=0.389, ppl=2.01, wps=33969.6, ups=0.74, wpb=46132.2, bsz=3212.3, num_updates=235000, lr=6.52328e-05, gnorm=0.184, clip=0, loss_scale=8, train_wall=136, gb_free=23.8, wall=15048
2022-11-02 09:36:59 | INFO | train_inner | epoch 015:  10760 / 16028 loss=1.586, nll_loss=1.007, nll_loss_teacher=0.538, kd_loss=0.356, ppl=2.01, wps=33835.3, ups=0.73, wpb=46229, bsz=3319.8, num_updates=235100, lr=6.52189e-05, gnorm=0.18, clip=0, loss_scale=8, train_wall=136, gb_free=22.6, wall=15185
2022-11-02 09:39:14 | INFO | train_inner | epoch 015:  10860 / 16028 loss=1.606, nll_loss=1.017, nll_loss_teacher=0.54, kd_loss=0.386, ppl=2.02, wps=34000.7, ups=0.74, wpb=45885.2, bsz=3049.4, num_updates=235200, lr=6.52051e-05, gnorm=0.193, clip=0, loss_scale=8, train_wall=135, gb_free=17.8, wall=15320
2022-11-02 09:41:32 | INFO | train_inner | epoch 015:  10960 / 16028 loss=1.592, nll_loss=1.013, nll_loss_teacher=0.544, kd_loss=0.362, ppl=2.02, wps=32758.3, ups=0.72, wpb=45434.4, bsz=3165.2, num_updates=235300, lr=6.51912e-05, gnorm=0.186, clip=0, loss_scale=8, train_wall=139, gb_free=28, wall=15459
2022-11-02 09:43:50 | INFO | train_inner | epoch 015:  11060 / 16028 loss=1.607, nll_loss=1.009, nll_loss_teacher=0.545, kd_loss=0.396, ppl=2.01, wps=33438.7, ups=0.73, wpb=46014.8, bsz=3237.5, num_updates=235400, lr=6.51774e-05, gnorm=0.18, clip=0, loss_scale=8, train_wall=137, gb_free=20.7, wall=15596
2022-11-02 09:46:06 | INFO | train_inner | epoch 015:  11160 / 16028 loss=1.61, nll_loss=1.01, nll_loss_teacher=0.54, kd_loss=0.4, ppl=2.01, wps=33708.4, ups=0.73, wpb=46033.7, bsz=3179.4, num_updates=235500, lr=6.51635e-05, gnorm=0.188, clip=0, loss_scale=8, train_wall=136, gb_free=21, wall=15733
2022-11-02 09:48:26 | INFO | train_inner | epoch 015:  11260 / 16028 loss=1.594, nll_loss=1.024, nll_loss_teacher=0.543, kd_loss=0.356, ppl=2.03, wps=32589.2, ups=0.72, wpb=45385.5, bsz=3264.2, num_updates=235600, lr=6.51497e-05, gnorm=0.186, clip=0, loss_scale=8, train_wall=139, gb_free=24.4, wall=15872
2022-11-02 09:50:44 | INFO | train_inner | epoch 015:  11360 / 16028 loss=1.595, nll_loss=1.005, nll_loss_teacher=0.546, kd_loss=0.374, ppl=2.01, wps=32616.1, ups=0.72, wpb=45211.4, bsz=3227, num_updates=235700, lr=6.51359e-05, gnorm=0.185, clip=0, loss_scale=8, train_wall=138, gb_free=23, wall=16011
2022-11-02 09:53:00 | INFO | train_inner | epoch 015:  11460 / 16028 loss=1.594, nll_loss=1.015, nll_loss_teacher=0.543, kd_loss=0.364, ppl=2.02, wps=33594.2, ups=0.73, wpb=45724.6, bsz=2999.7, num_updates=235800, lr=6.51221e-05, gnorm=0.184, clip=0, loss_scale=8, train_wall=136, gb_free=22.6, wall=16147
2022-11-02 09:55:19 | INFO | train_inner | epoch 015:  11560 / 16028 loss=1.619, nll_loss=1.009, nll_loss_teacher=0.54, kd_loss=0.421, ppl=2.01, wps=33132.8, ups=0.72, wpb=45763.6, bsz=3172.4, num_updates=235900, lr=6.51083e-05, gnorm=0.192, clip=0, loss_scale=8, train_wall=138, gb_free=25.1, wall=16285
2022-11-02 09:57:37 | INFO | train_inner | epoch 015:  11660 / 16028 loss=1.608, nll_loss=1.026, nll_loss_teacher=0.544, kd_loss=0.382, ppl=2.04, wps=33101, ups=0.72, wpb=45709.6, bsz=3113.4, num_updates=236000, lr=6.50945e-05, gnorm=0.197, clip=0, loss_scale=8, train_wall=138, gb_free=24.1, wall=16423
2022-11-02 09:59:54 | INFO | train_inner | epoch 015:  11760 / 16028 loss=1.607, nll_loss=1.018, nll_loss_teacher=0.548, kd_loss=0.386, ppl=2.02, wps=33258.7, ups=0.73, wpb=45547.2, bsz=3034.6, num_updates=236100, lr=6.50807e-05, gnorm=0.182, clip=0, loss_scale=8, train_wall=137, gb_free=23.1, wall=16560
2022-11-02 10:02:11 | INFO | train_inner | epoch 015:  11860 / 16028 loss=1.59, nll_loss=1.011, nll_loss_teacher=0.541, kd_loss=0.359, ppl=2.02, wps=33236.6, ups=0.73, wpb=45525.5, bsz=3134.8, num_updates=236200, lr=6.50669e-05, gnorm=0.186, clip=0, loss_scale=8, train_wall=137, gb_free=24, wall=16697
2022-11-02 10:04:25 | INFO | train_inner | epoch 015:  11960 / 16028 loss=1.596, nll_loss=1.01, nll_loss_teacher=0.544, kd_loss=0.372, ppl=2.01, wps=34258.2, ups=0.74, wpb=46200.6, bsz=3160.7, num_updates=236300, lr=6.50531e-05, gnorm=0.183, clip=0, loss_scale=8, train_wall=135, gb_free=22.6, wall=16832
2022-11-02 10:06:43 | INFO | train_inner | epoch 015:  12060 / 16028 loss=1.634, nll_loss=1.018, nll_loss_teacher=0.545, kd_loss=0.441, ppl=2.03, wps=33464.8, ups=0.72, wpb=46189.9, bsz=2999.1, num_updates=236400, lr=6.50394e-05, gnorm=0.189, clip=0, loss_scale=8, train_wall=138, gb_free=16.8, wall=16970
2022-11-02 10:09:01 | INFO | train_inner | epoch 015:  12160 / 16028 loss=1.619, nll_loss=1.014, nll_loss_teacher=0.54, kd_loss=0.414, ppl=2.02, wps=33596.3, ups=0.72, wpb=46349, bsz=3089.8, num_updates=236500, lr=6.50256e-05, gnorm=0.187, clip=0, loss_scale=8, train_wall=138, gb_free=19.6, wall=17108
2022-11-02 10:11:17 | INFO | train_inner | epoch 015:  12260 / 16028 loss=1.587, nll_loss=1.01, nll_loss_teacher=0.544, kd_loss=0.354, ppl=2.01, wps=33977.2, ups=0.74, wpb=45906.5, bsz=3161.1, num_updates=236600, lr=6.50119e-05, gnorm=0.181, clip=0, loss_scale=8, train_wall=135, gb_free=23, wall=17243
2022-11-02 10:13:32 | INFO | train_inner | epoch 015:  12360 / 16028 loss=1.612, nll_loss=1.006, nll_loss_teacher=0.542, kd_loss=0.409, ppl=2.01, wps=34364.1, ups=0.74, wpb=46422.6, bsz=3040.3, num_updates=236700, lr=6.49981e-05, gnorm=0.185, clip=0, loss_scale=8, train_wall=135, gb_free=19.2, wall=17378
2022-11-02 10:15:50 | INFO | train_inner | epoch 015:  12460 / 16028 loss=1.585, nll_loss=1.014, nll_loss_teacher=0.542, kd_loss=0.346, ppl=2.02, wps=33008.3, ups=0.72, wpb=45626.3, bsz=3327.8, num_updates=236800, lr=6.49844e-05, gnorm=0.185, clip=0, loss_scale=8, train_wall=138, gb_free=25, wall=17516
2022-11-02 10:18:09 | INFO | train_inner | epoch 015:  12560 / 16028 loss=1.575, nll_loss=1.013, nll_loss_teacher=0.541, kd_loss=0.328, ppl=2.02, wps=33052.9, ups=0.72, wpb=45996.8, bsz=3055, num_updates=236900, lr=6.49707e-05, gnorm=0.189, clip=0, loss_scale=8, train_wall=139, gb_free=21.4, wall=17655
2022-11-02 10:20:23 | INFO | train_inner | epoch 015:  12660 / 16028 loss=1.606, nll_loss=1.013, nll_loss_teacher=0.544, kd_loss=0.39, ppl=2.02, wps=34147.7, ups=0.75, wpb=45582.6, bsz=3164.6, num_updates=237000, lr=6.4957e-05, gnorm=0.195, clip=0, loss_scale=8, train_wall=133, gb_free=22.6, wall=17789
2022-11-02 10:22:39 | INFO | train_inner | epoch 015:  12760 / 16028 loss=1.6, nll_loss=1.01, nll_loss_teacher=0.542, kd_loss=0.381, ppl=2.01, wps=33527.6, ups=0.73, wpb=45871.4, bsz=3083, num_updates=237100, lr=6.49433e-05, gnorm=0.185, clip=0, loss_scale=8, train_wall=137, gb_free=16.5, wall=17926
2022-11-02 10:24:57 | INFO | train_inner | epoch 015:  12860 / 16028 loss=1.628, nll_loss=1.012, nll_loss_teacher=0.543, kd_loss=0.436, ppl=2.02, wps=32959.4, ups=0.73, wpb=45439.3, bsz=3042.3, num_updates=237200, lr=6.49296e-05, gnorm=0.191, clip=0, loss_scale=8, train_wall=138, gb_free=22.5, wall=18064
2022-11-02 10:27:14 | INFO | train_inner | epoch 015:  12960 / 16028 loss=1.615, nll_loss=1.01, nll_loss_teacher=0.545, kd_loss=0.41, ppl=2.01, wps=33705.6, ups=0.73, wpb=46155.7, bsz=3075.4, num_updates=237300, lr=6.49159e-05, gnorm=0.183, clip=0, loss_scale=8, train_wall=137, gb_free=25, wall=18200
2022-11-02 10:29:33 | INFO | train_inner | epoch 015:  13060 / 16028 loss=1.61, nll_loss=1.02, nll_loss_teacher=0.539, kd_loss=0.391, ppl=2.03, wps=33085.5, ups=0.72, wpb=45950.2, bsz=3084.4, num_updates=237400, lr=6.49022e-05, gnorm=0.195, clip=0, loss_scale=8, train_wall=139, gb_free=18.7, wall=18339
2022-11-02 10:31:50 | INFO | train_inner | epoch 015:  13160 / 16028 loss=1.598, nll_loss=1.011, nll_loss_teacher=0.541, kd_loss=0.375, ppl=2.02, wps=33589.4, ups=0.73, wpb=46147.6, bsz=3058.5, num_updates=237500, lr=6.48886e-05, gnorm=0.189, clip=0, loss_scale=8, train_wall=137, gb_free=19.8, wall=18477
2022-11-02 10:34:10 | INFO | train_inner | epoch 015:  13260 / 16028 loss=1.593, nll_loss=1.01, nll_loss_teacher=0.543, kd_loss=0.367, ppl=2.01, wps=32863.2, ups=0.72, wpb=45720.8, bsz=3117.3, num_updates=237600, lr=6.48749e-05, gnorm=0.184, clip=0, loss_scale=8, train_wall=139, gb_free=20.2, wall=18616
2022-11-02 10:36:27 | INFO | train_inner | epoch 015:  13360 / 16028 loss=1.588, nll_loss=1.009, nll_loss_teacher=0.541, kd_loss=0.357, ppl=2.01, wps=33397, ups=0.73, wpb=45759.9, bsz=3039.3, num_updates=237700, lr=6.48613e-05, gnorm=0.188, clip=0, loss_scale=8, train_wall=137, gb_free=18.6, wall=18753
2022-11-02 10:38:43 | INFO | train_inner | epoch 015:  13460 / 16028 loss=1.598, nll_loss=1.011, nll_loss_teacher=0.545, kd_loss=0.376, ppl=2.01, wps=32777.1, ups=0.73, wpb=44831.1, bsz=3061.4, num_updates=237800, lr=6.48476e-05, gnorm=0.185, clip=0, loss_scale=8, train_wall=137, gb_free=25.2, wall=18890
2022-11-02 10:40:57 | INFO | train_inner | epoch 015:  13560 / 16028 loss=1.611, nll_loss=1.012, nll_loss_teacher=0.541, kd_loss=0.401, ppl=2.02, wps=34302.3, ups=0.75, wpb=45982.9, bsz=3199.5, num_updates=237900, lr=6.4834e-05, gnorm=0.188, clip=0, loss_scale=8, train_wall=134, gb_free=26.6, wall=19024
2022-11-02 10:43:16 | INFO | train_inner | epoch 015:  13660 / 16028 loss=1.606, nll_loss=1.015, nll_loss_teacher=0.538, kd_loss=0.387, ppl=2.02, wps=32746.5, ups=0.72, wpb=45304.9, bsz=3122.2, num_updates=238000, lr=6.48204e-05, gnorm=0.19, clip=0, loss_scale=8, train_wall=138, gb_free=23.7, wall=19162
2022-11-02 10:45:32 | INFO | train_inner | epoch 015:  13760 / 16028 loss=1.616, nll_loss=1.009, nll_loss_teacher=0.543, kd_loss=0.413, ppl=2.01, wps=33320.1, ups=0.73, wpb=45481.5, bsz=3130.2, num_updates=238100, lr=6.48068e-05, gnorm=0.186, clip=0, loss_scale=8, train_wall=136, gb_free=20.2, wall=19299
2022-11-02 10:47:49 | INFO | train_inner | epoch 015:  13860 / 16028 loss=1.601, nll_loss=1.009, nll_loss_teacher=0.544, kd_loss=0.383, ppl=2.01, wps=33708.2, ups=0.73, wpb=46025.9, bsz=3078.3, num_updates=238200, lr=6.47932e-05, gnorm=0.187, clip=0, loss_scale=8, train_wall=136, gb_free=23.4, wall=19435
2022-11-02 10:50:06 | INFO | train_inner | epoch 015:  13960 / 16028 loss=1.591, nll_loss=1.003, nll_loss_teacher=0.544, kd_loss=0.37, ppl=2, wps=33870.4, ups=0.73, wpb=46347.3, bsz=3173.8, num_updates=238300, lr=6.47796e-05, gnorm=0.182, clip=0, loss_scale=16, train_wall=137, gb_free=24.1, wall=19572
2022-11-02 10:52:23 | INFO | train_inner | epoch 015:  14060 / 16028 loss=1.592, nll_loss=1.012, nll_loss_teacher=0.54, kd_loss=0.363, ppl=2.02, wps=33192.4, ups=0.73, wpb=45616.3, bsz=3142.6, num_updates=238400, lr=6.4766e-05, gnorm=0.184, clip=0, loss_scale=16, train_wall=137, gb_free=20.5, wall=19709
2022-11-02 10:54:41 | INFO | train_inner | epoch 015:  14160 / 16028 loss=1.602, nll_loss=1.012, nll_loss_teacher=0.544, kd_loss=0.383, ppl=2.02, wps=32655.2, ups=0.72, wpb=45055.7, bsz=3136.8, num_updates=238500, lr=6.47524e-05, gnorm=0.189, clip=0, loss_scale=16, train_wall=138, gb_free=26.2, wall=19847
2022-11-02 10:56:56 | INFO | train_inner | epoch 015:  14260 / 16028 loss=1.597, nll_loss=1.014, nll_loss_teacher=0.54, kd_loss=0.371, ppl=2.02, wps=33786.2, ups=0.74, wpb=45552.6, bsz=2983.8, num_updates=238600, lr=6.47388e-05, gnorm=0.188, clip=0, loss_scale=16, train_wall=135, gb_free=26.8, wall=19982
2022-11-02 10:59:16 | INFO | train_inner | epoch 015:  14360 / 16028 loss=1.603, nll_loss=1.01, nll_loss_teacher=0.547, kd_loss=0.386, ppl=2.01, wps=32330.4, ups=0.71, wpb=45298, bsz=3104.1, num_updates=238700, lr=6.47253e-05, gnorm=0.195, clip=0, loss_scale=16, train_wall=140, gb_free=17.6, wall=20122
2022-11-02 11:01:34 | INFO | train_inner | epoch 015:  14460 / 16028 loss=1.599, nll_loss=1.016, nll_loss_teacher=0.539, kd_loss=0.373, ppl=2.02, wps=32767.8, ups=0.72, wpb=45216.6, bsz=3016.5, num_updates=238800, lr=6.47117e-05, gnorm=0.189, clip=0, loss_scale=16, train_wall=138, gb_free=23.7, wall=20260
2022-11-02 11:03:49 | INFO | train_inner | epoch 015:  14560 / 16028 loss=1.587, nll_loss=1.009, nll_loss_teacher=0.543, kd_loss=0.356, ppl=2.01, wps=33741.4, ups=0.74, wpb=45709.9, bsz=3230.4, num_updates=238900, lr=6.46982e-05, gnorm=0.181, clip=0, loss_scale=16, train_wall=135, gb_free=20.4, wall=20396
2022-11-02 11:06:09 | INFO | train_inner | epoch 015:  14660 / 16028 loss=1.612, nll_loss=1.016, nll_loss_teacher=0.545, kd_loss=0.398, ppl=2.02, wps=33110.6, ups=0.72, wpb=46140.3, bsz=2953.4, num_updates=239000, lr=6.46846e-05, gnorm=0.189, clip=0, loss_scale=16, train_wall=139, gb_free=21.7, wall=20535
2022-11-02 11:08:25 | INFO | train_inner | epoch 015:  14760 / 16028 loss=1.59, nll_loss=1.017, nll_loss_teacher=0.546, kd_loss=0.353, ppl=2.02, wps=33599.9, ups=0.73, wpb=45736.7, bsz=2957.2, num_updates=239100, lr=6.46711e-05, gnorm=0.187, clip=0, loss_scale=16, train_wall=136, gb_free=20.6, wall=20671
2022-11-02 11:10:43 | INFO | train_inner | epoch 015:  14860 / 16028 loss=1.591, nll_loss=1.017, nll_loss_teacher=0.542, kd_loss=0.356, ppl=2.02, wps=32969.1, ups=0.72, wpb=45549.3, bsz=3034.3, num_updates=239200, lr=6.46576e-05, gnorm=0.187, clip=0, loss_scale=16, train_wall=138, gb_free=23.1, wall=20809
2022-11-02 11:12:58 | INFO | train_inner | epoch 015:  14960 / 16028 loss=1.623, nll_loss=1.015, nll_loss_teacher=0.545, kd_loss=0.423, ppl=2.02, wps=33794.2, ups=0.74, wpb=45433.7, bsz=3137.7, num_updates=239300, lr=6.46441e-05, gnorm=0.188, clip=0, loss_scale=16, train_wall=134, gb_free=27.9, wall=20944
2022-11-02 11:15:12 | INFO | train_inner | epoch 015:  15060 / 16028 loss=1.607, nll_loss=1.017, nll_loss_teacher=0.544, kd_loss=0.388, ppl=2.02, wps=34331.3, ups=0.75, wpb=46029.4, bsz=3134.8, num_updates=239400, lr=6.46306e-05, gnorm=0.185, clip=0, loss_scale=16, train_wall=134, gb_free=16.8, wall=21078
2022-11-02 11:17:28 | INFO | train_inner | epoch 015:  15160 / 16028 loss=1.603, nll_loss=1.02, nll_loss_teacher=0.544, kd_loss=0.377, ppl=2.03, wps=33966.3, ups=0.73, wpb=46350.8, bsz=3049.4, num_updates=239500, lr=6.46171e-05, gnorm=0.183, clip=0, loss_scale=16, train_wall=136, gb_free=20.5, wall=21214
2022-11-02 11:18:42 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-11-02 11:19:45 | INFO | train_inner | epoch 015:  15261 / 16028 loss=1.588, nll_loss=1.012, nll_loss_teacher=0.546, kd_loss=0.355, ppl=2.02, wps=33034.3, ups=0.73, wpb=45279.6, bsz=3133.3, num_updates=239600, lr=6.46036e-05, gnorm=0.183, clip=0, loss_scale=8, train_wall=137, gb_free=25.8, wall=21351
2022-11-02 11:22:02 | INFO | train_inner | epoch 015:  15361 / 16028 loss=1.591, nll_loss=1.008, nll_loss_teacher=0.542, kd_loss=0.364, ppl=2.01, wps=32971.5, ups=0.73, wpb=45125, bsz=3112.5, num_updates=239700, lr=6.45901e-05, gnorm=0.184, clip=0, loss_scale=8, train_wall=137, gb_free=23.1, wall=21488
2022-11-02 11:24:21 | INFO | train_inner | epoch 015:  15461 / 16028 loss=1.602, nll_loss=1.012, nll_loss_teacher=0.539, kd_loss=0.383, ppl=2.02, wps=32930.2, ups=0.72, wpb=45696.6, bsz=3125.8, num_updates=239800, lr=6.45766e-05, gnorm=0.188, clip=0, loss_scale=8, train_wall=139, gb_free=22.6, wall=21627
2022-11-02 11:26:36 | INFO | train_inner | epoch 015:  15561 / 16028 loss=1.607, nll_loss=1, nll_loss_teacher=0.54, kd_loss=0.405, ppl=2, wps=34411.9, ups=0.74, wpb=46435.6, bsz=3321.5, num_updates=239900, lr=6.45632e-05, gnorm=0.189, clip=0, loss_scale=8, train_wall=135, gb_free=22.1, wall=21762
2022-11-02 11:28:51 | INFO | train_inner | epoch 015:  15661 / 16028 loss=1.601, nll_loss=1, nll_loss_teacher=0.541, kd_loss=0.393, ppl=2, wps=34542.5, ups=0.74, wpb=46852.1, bsz=3226.7, num_updates=240000, lr=6.45497e-05, gnorm=0.18, clip=0, loss_scale=8, train_wall=135, gb_free=20.2, wall=21898
2022-11-02 11:31:10 | INFO | train_inner | epoch 015:  15761 / 16028 loss=1.585, nll_loss=1.013, nll_loss_teacher=0.542, kd_loss=0.348, ppl=2.02, wps=32514.6, ups=0.72, wpb=45155.6, bsz=3210.1, num_updates=240100, lr=6.45363e-05, gnorm=0.182, clip=0, loss_scale=8, train_wall=139, gb_free=23.9, wall=22037
2022-11-02 11:33:30 | INFO | train_inner | epoch 015:  15861 / 16028 loss=1.606, nll_loss=1.015, nll_loss_teacher=0.544, kd_loss=0.388, ppl=2.02, wps=33175.6, ups=0.72, wpb=46228.1, bsz=2813.7, num_updates=240200, lr=6.45228e-05, gnorm=0.186, clip=0, loss_scale=8, train_wall=139, gb_free=16.7, wall=22176
2022-11-02 11:35:47 | INFO | train_inner | epoch 015:  15961 / 16028 loss=1.587, nll_loss=1.005, nll_loss_teacher=0.542, kd_loss=0.359, ppl=2.01, wps=32823, ups=0.73, wpb=45030.2, bsz=3236.2, num_updates=240300, lr=6.45094e-05, gnorm=0.181, clip=0, loss_scale=8, train_wall=137, gb_free=24.6, wall=22313
2022-11-02 11:37:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-11-02 11:41:33 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 3.046 | nll_loss 2.984 | nll_loss_teacher 2.67 | kd_loss 1.478 | ppl 7.91 | wps 44007.3 | wpb 34870.6 | bsz 2819.3 | num_updates 240367 | best_loss 3.045
2022-11-02 11:41:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 15 @ 240367 updates
2022-11-02 11:41:33 | INFO | fairseq.trainer | Saving checkpoint to /nlsasfs/home/ai4bharat/yashkm/varun/Indic-En-Distillation/checkpoints/global-multi-distil/checkpoint15.pt
2022-11-02 11:41:35 | INFO | fairseq.trainer | Finished saving checkpoint to /nlsasfs/home/ai4bharat/yashkm/varun/Indic-En-Distillation/checkpoints/global-multi-distil/checkpoint15.pt
2022-11-02 11:41:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/global-multi-distil/checkpoint15.pt (epoch 15 @ 240367 updates, score 3.046) (writing took 3.7825422417372465 seconds)
2022-11-02 11:41:37 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2022-11-02 11:41:37 | INFO | train | epoch 015 | loss 1.601 | nll_loss 1.011 | nll_loss_teacher 0.543 | kd_loss 0.382 | ppl 2.02 | wps 33152.9 | ups 0.72 | wpb 45832.4 | bsz 3104.8 | num_updates 240367 | lr 6.45004e-05 | gnorm 0.185 | clip 0 | loss_scale 8 | train_wall 21884 | gb_free 20.8 | wall 22664
2022-11-02 11:41:37 | INFO | fairseq.data.iterators | grouped total_num_itrs = 16028
2022-11-02 11:41:38 | INFO | fairseq.trainer | begin training epoch 16
2022-11-02 11:41:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-11-02 11:49:41 | INFO | train_inner | epoch 016:     33 / 16028 loss=1.629, nll_loss=1.014, nll_loss_teacher=0.541, kd_loss=0.435, ppl=2.02, wps=5454.7, ups=0.12, wpb=45514.2, bsz=3090.5, num_updates=240400, lr=6.4496e-05, gnorm=0.191, clip=0, loss_scale=8, train_wall=157, gb_free=20.9, wall=23148
2022-11-02 11:52:00 | INFO | train_inner | epoch 016:    133 / 16028 loss=1.597, nll_loss=1.004, nll_loss_teacher=0.541, kd_loss=0.379, ppl=2.01, wps=32936.2, ups=0.72, wpb=45536.5, bsz=2997.3, num_updates=240500, lr=6.44826e-05, gnorm=0.186, clip=0, loss_scale=8, train_wall=138, gb_free=21.8, wall=23286
2022-11-02 11:54:15 | INFO | train_inner | epoch 016:    233 / 16028 loss=1.595, nll_loss=0.995, nll_loss_teacher=0.54, kd_loss=0.384, ppl=1.99, wps=33859.1, ups=0.74, wpb=45919, bsz=3186.1, num_updates=240600, lr=6.44692e-05, gnorm=0.187, clip=0, loss_scale=8, train_wall=135, gb_free=26.7, wall=23422
2022-11-02 11:56:30 | INFO | train_inner | epoch 016:    333 / 16028 loss=1.593, nll_loss=0.997, nll_loss_teacher=0.543, kd_loss=0.379, ppl=2, wps=34049.1, ups=0.74, wpb=46039.7, bsz=3176.6, num_updates=240700, lr=6.44558e-05, gnorm=0.182, clip=0, loss_scale=8, train_wall=135, gb_free=18.4, wall=23557
2022-11-02 11:58:49 | INFO | train_inner | epoch 016:    433 / 16028 loss=1.601, nll_loss=1.003, nll_loss_teacher=0.543, kd_loss=0.389, ppl=2, wps=32462.3, ups=0.72, wpb=44881.5, bsz=3152.6, num_updates=240800, lr=6.44424e-05, gnorm=0.184, clip=0, loss_scale=8, train_wall=138, gb_free=25.4, wall=23695
2022-11-02 12:01:06 | INFO | train_inner | epoch 016:    533 / 16028 loss=1.603, nll_loss=1, nll_loss_teacher=0.541, kd_loss=0.396, ppl=2, wps=33168.2, ups=0.73, wpb=45637.9, bsz=3084.6, num_updates=240900, lr=6.4429e-05, gnorm=0.184, clip=0, loss_scale=8, train_wall=137, gb_free=29.7, wall=23833
2022-11-02 12:03:25 | INFO | train_inner | epoch 016:    633 / 16028 loss=1.612, nll_loss=1.003, nll_loss_teacher=0.543, kd_loss=0.41, ppl=2, wps=32131.9, ups=0.72, wpb=44458.1, bsz=3154.6, num_updates=241000, lr=6.44157e-05, gnorm=0.188, clip=0, loss_scale=8, train_wall=138, gb_free=21.9, wall=23971
2022-11-02 12:05:40 | INFO | train_inner | epoch 016:    733 / 16028 loss=1.608, nll_loss=1.002, nll_loss_teacher=0.541, kd_loss=0.403, ppl=2, wps=34193.1, ups=0.74, wpb=46304.7, bsz=3253.4, num_updates=241100, lr=6.44023e-05, gnorm=0.186, clip=0, loss_scale=8, train_wall=135, gb_free=19.2, wall=24106
2022-11-02 12:07:57 | INFO | train_inner | epoch 016:    833 / 16028 loss=1.582, nll_loss=1.002, nll_loss_teacher=0.54, kd_loss=0.352, ppl=2, wps=33253.9, ups=0.73, wpb=45661.6, bsz=3296.4, num_updates=241200, lr=6.4389e-05, gnorm=0.179, clip=0, loss_scale=8, train_wall=137, gb_free=22.3, wall=24244
2022-11-02 12:10:14 | INFO | train_inner | epoch 016:    933 / 16028 loss=1.594, nll_loss=0.988, nll_loss_teacher=0.54, kd_loss=0.388, ppl=1.98, wps=33577.4, ups=0.73, wpb=45917.6, bsz=3188.8, num_updates=241300, lr=6.43756e-05, gnorm=0.182, clip=0, loss_scale=8, train_wall=137, gb_free=16.7, wall=24380
2022-11-02 12:12:29 | INFO | train_inner | epoch 016:   1033 / 16028 loss=1.599, nll_loss=1, nll_loss_teacher=0.54, kd_loss=0.388, ppl=2, wps=34267.5, ups=0.74, wpb=46208.9, bsz=3097.3, num_updates=241400, lr=6.43623e-05, gnorm=0.177, clip=0, loss_scale=8, train_wall=135, gb_free=22.1, wall=24515
2022-11-02 12:14:49 | INFO | train_inner | epoch 016:   1133 / 16028 loss=1.594, nll_loss=1.009, nll_loss_teacher=0.543, kd_loss=0.368, ppl=2.01, wps=32592.5, ups=0.72, wpb=45499.1, bsz=2998.6, num_updates=241500, lr=6.43489e-05, gnorm=0.189, clip=0, loss_scale=8, train_wall=139, gb_free=19.1, wall=24655
2022-11-02 12:17:05 | INFO | train_inner | epoch 016:   1233 / 16028 loss=1.603, nll_loss=0.993, nll_loss_teacher=0.54, kd_loss=0.404, ppl=1.99, wps=33670.1, ups=0.73, wpb=45990.2, bsz=3227.7, num_updates=241600, lr=6.43356e-05, gnorm=0.184, clip=0, loss_scale=8, train_wall=136, gb_free=22.3, wall=24792
2022-11-02 12:19:21 | INFO | train_inner | epoch 016:   1333 / 16028 loss=1.607, nll_loss=1.009, nll_loss_teacher=0.544, kd_loss=0.394, ppl=2.01, wps=33635.1, ups=0.74, wpb=45592.6, bsz=2966.7, num_updates=241700, lr=6.43223e-05, gnorm=0.184, clip=0, loss_scale=8, train_wall=135, gb_free=28.2, wall=24927
2022-11-02 12:21:39 | INFO | train_inner | epoch 016:   1433 / 16028 loss=1.587, nll_loss=1.007, nll_loss_teacher=0.543, kd_loss=0.356, ppl=2.01, wps=32672.7, ups=0.73, wpb=45012.2, bsz=3070.6, num_updates=241800, lr=6.4309e-05, gnorm=0.19, clip=0, loss_scale=8, train_wall=138, gb_free=17, wall=25065
2022-11-02 12:23:57 | INFO | train_inner | epoch 016:   1533 / 16028 loss=1.585, nll_loss=1.004, nll_loss_teacher=0.543, kd_loss=0.355, ppl=2.01, wps=33249.3, ups=0.72, wpb=46001.3, bsz=2961.3, num_updates=241900, lr=6.42957e-05, gnorm=0.183, clip=0, loss_scale=8, train_wall=138, gb_free=25.2, wall=25203
2022-11-02 12:26:13 | INFO | train_inner | epoch 016:   1633 / 16028 loss=1.609, nll_loss=1.002, nll_loss_teacher=0.54, kd_loss=0.406, ppl=2, wps=33332.2, ups=0.73, wpb=45448, bsz=3195.1, num_updates=242000, lr=6.42824e-05, gnorm=0.191, clip=0, loss_scale=8, train_wall=136, gb_free=19.7, wall=25340
2022-11-02 12:28:28 | INFO | train_inner | epoch 016:   1733 / 16028 loss=1.596, nll_loss=1.004, nll_loss_teacher=0.541, kd_loss=0.378, ppl=2.01, wps=33526.5, ups=0.74, wpb=45199.8, bsz=3255.4, num_updates=242100, lr=6.42692e-05, gnorm=0.188, clip=0, loss_scale=8, train_wall=135, gb_free=22.8, wall=25474
2022-11-02 12:30:46 | INFO | train_inner | epoch 016:   1833 / 16028 loss=1.603, nll_loss=1.016, nll_loss_teacher=0.545, kd_loss=0.379, ppl=2.02, wps=32626.3, ups=0.73, wpb=44911.2, bsz=2881.7, num_updates=242200, lr=6.42559e-05, gnorm=0.193, clip=0, loss_scale=8, train_wall=137, gb_free=20.6, wall=25612
2022-11-02 12:33:04 | INFO | train_inner | epoch 016:   1933 / 16028 loss=1.602, nll_loss=1.009, nll_loss_teacher=0.542, kd_loss=0.384, ppl=2.01, wps=33211.4, ups=0.72, wpb=45884.6, bsz=3029.4, num_updates=242300, lr=6.42426e-05, gnorm=0.188, clip=0, loss_scale=8, train_wall=138, gb_free=20.8, wall=25750
2022-11-02 12:35:21 | INFO | train_inner | epoch 016:   2033 / 16028 loss=1.615, nll_loss=1.013, nll_loss_teacher=0.543, kd_loss=0.407, ppl=2.02, wps=33712.4, ups=0.73, wpb=46175.9, bsz=3075, num_updates=242400, lr=6.42294e-05, gnorm=0.188, clip=0, loss_scale=8, train_wall=137, gb_free=23.5, wall=25887
2022-11-02 12:37:37 | INFO | train_inner | epoch 016:   2133 / 16028 loss=1.584, nll_loss=1.004, nll_loss_teacher=0.54, kd_loss=0.354, ppl=2.01, wps=34167.8, ups=0.74, wpb=46399.6, bsz=2894.6, num_updates=242500, lr=6.42161e-05, gnorm=0.182, clip=0, loss_scale=8, train_wall=136, gb_free=20.6, wall=26023
2022-11-02 12:39:55 | INFO | train_inner | epoch 016:   2233 / 16028 loss=1.6, nll_loss=1.014, nll_loss_teacher=0.541, kd_loss=0.376, ppl=2.02, wps=32670, ups=0.72, wpb=45120.6, bsz=3061.7, num_updates=242600, lr=6.42029e-05, gnorm=0.186, clip=0, loss_scale=8, train_wall=138, gb_free=16.9, wall=26161
2022-11-02 12:42:13 | INFO | train_inner | epoch 016:   2333 / 16028 loss=1.595, nll_loss=1.001, nll_loss_teacher=0.542, kd_loss=0.377, ppl=2, wps=32816.4, ups=0.73, wpb=45222.1, bsz=3007.3, num_updates=242700, lr=6.41897e-05, gnorm=0.186, clip=0, loss_scale=8, train_wall=138, gb_free=16.9, wall=26299
2022-11-02 12:44:30 | INFO | train_inner | epoch 016:   2433 / 16028 loss=1.59, nll_loss=1.009, nll_loss_teacher=0.541, kd_loss=0.359, ppl=2.01, wps=33473.4, ups=0.73, wpb=46122, bsz=3123.4, num_updates=242800, lr=6.41764e-05, gnorm=0.185, clip=0, loss_scale=8, train_wall=138, gb_free=18.8, wall=26437
2022-11-02 12:46:49 | INFO | train_inner | epoch 016:   2533 / 16028 loss=1.607, nll_loss=1.009, nll_loss_teacher=0.541, kd_loss=0.396, ppl=2.01, wps=33455.4, ups=0.72, wpb=46273.6, bsz=3088.6, num_updates=242900, lr=6.41632e-05, gnorm=0.181, clip=0, loss_scale=8, train_wall=138, gb_free=19.9, wall=26575
2022-11-02 12:49:09 | INFO | train_inner | epoch 016:   2633 / 16028 loss=1.592, nll_loss=1.007, nll_loss_teacher=0.539, kd_loss=0.367, ppl=2.01, wps=32597.3, ups=0.71, wpb=45851.8, bsz=3078.2, num_updates=243000, lr=6.415e-05, gnorm=0.186, clip=0, loss_scale=8, train_wall=140, gb_free=18.7, wall=26716
2022-11-02 12:51:25 | INFO | train_inner | epoch 016:   2733 / 16028 loss=1.596, nll_loss=1, nll_loss_teacher=0.541, kd_loss=0.381, ppl=2, wps=34180.6, ups=0.73, wpb=46531.7, bsz=3154.6, num_updates=243100, lr=6.41368e-05, gnorm=0.182, clip=0, loss_scale=8, train_wall=136, gb_free=24, wall=26852
2022-11-02 12:53:45 | INFO | train_inner | epoch 016:   2833 / 16028 loss=1.607, nll_loss=1.003, nll_loss_teacher=0.544, kd_loss=0.401, ppl=2, wps=32840.5, ups=0.71, wpb=45973.8, bsz=3082.6, num_updates=243200, lr=6.41236e-05, gnorm=0.187, clip=0, loss_scale=8, train_wall=140, gb_free=25.3, wall=26992
2022-11-02 12:56:05 | INFO | train_inner | epoch 016:   2933 / 16028 loss=1.609, nll_loss=1.002, nll_loss_teacher=0.545, kd_loss=0.406, ppl=2, wps=32530.2, ups=0.72, wpb=45343, bsz=3073, num_updates=243300, lr=6.41105e-05, gnorm=0.182, clip=0, loss_scale=8, train_wall=139, gb_free=23.1, wall=27131
2022-11-02 12:58:20 | INFO | train_inner | epoch 016:   3033 / 16028 loss=1.585, nll_loss=1.011, nll_loss_teacher=0.544, kd_loss=0.349, ppl=2.02, wps=33981, ups=0.74, wpb=45798.1, bsz=3083.3, num_updates=243400, lr=6.40973e-05, gnorm=0.181, clip=0, loss_scale=8, train_wall=135, gb_free=20.4, wall=27266
2022-11-02 13:00:36 | INFO | train_inner | epoch 016:   3133 / 16028 loss=1.587, nll_loss=1.009, nll_loss_teacher=0.547, kd_loss=0.354, ppl=2.01, wps=33434.5, ups=0.73, wpb=45677.6, bsz=3121.7, num_updates=243500, lr=6.40841e-05, gnorm=0.183, clip=0, loss_scale=8, train_wall=136, gb_free=24, wall=27403
2022-11-02 13:02:53 | INFO | train_inner | epoch 016:   3233 / 16028 loss=1.607, nll_loss=1, nll_loss_teacher=0.54, kd_loss=0.405, ppl=2, wps=33773.8, ups=0.73, wpb=46240.8, bsz=3119.4, num_updates=243600, lr=6.4071e-05, gnorm=0.184, clip=0, loss_scale=8, train_wall=137, gb_free=17.8, wall=27539
2022-11-02 13:05:13 | INFO | train_inner | epoch 016:   3333 / 16028 loss=1.615, nll_loss=1.002, nll_loss_teacher=0.543, kd_loss=0.417, ppl=2, wps=32997.4, ups=0.72, wpb=46034.8, bsz=3005, num_updates=243700, lr=6.40578e-05, gnorm=0.184, clip=0, loss_scale=16, train_wall=139, gb_free=20.4, wall=27679
2022-11-02 13:07:33 | INFO | train_inner | epoch 016:   3433 / 16028 loss=1.566, nll_loss=0.999, nll_loss_teacher=0.543, kd_loss=0.321, ppl=2, wps=32728.1, ups=0.71, wpb=45990.1, bsz=3253.2, num_updates=243800, lr=6.40447e-05, gnorm=0.181, clip=0, loss_scale=16, train_wall=140, gb_free=21, wall=27820
2022-11-02 13:09:52 | INFO | train_inner | epoch 016:   3533 / 16028 loss=1.592, nll_loss=1.002, nll_loss_teacher=0.542, kd_loss=0.372, ppl=2, wps=32921.1, ups=0.72, wpb=45791.8, bsz=3154.6, num_updates=243900, lr=6.40316e-05, gnorm=0.182, clip=0, loss_scale=16, train_wall=139, gb_free=20.6, wall=27959
2022-11-02 13:12:11 | INFO | train_inner | epoch 016:   3633 / 16028 loss=1.598, nll_loss=1.007, nll_loss_teacher=0.543, kd_loss=0.378, ppl=2.01, wps=33140.4, ups=0.72, wpb=45850.5, bsz=3076.8, num_updates=244000, lr=6.40184e-05, gnorm=0.184, clip=0, loss_scale=16, train_wall=138, gb_free=21.7, wall=28097
2022-11-02 13:14:27 | INFO | train_inner | epoch 016:   3733 / 16028 loss=1.584, nll_loss=1.006, nll_loss_teacher=0.542, kd_loss=0.351, ppl=2.01, wps=34285.1, ups=0.74, wpb=46641.2, bsz=3167.4, num_updates=244100, lr=6.40053e-05, gnorm=0.182, clip=0, loss_scale=16, train_wall=136, gb_free=21.4, wall=28233
2022-11-02 13:16:43 | INFO | train_inner | epoch 016:   3833 / 16028 loss=1.602, nll_loss=1.006, nll_loss_teacher=0.543, kd_loss=0.387, ppl=2.01, wps=34081.4, ups=0.74, wpb=46368.9, bsz=3117.7, num_updates=244200, lr=6.39922e-05, gnorm=0.189, clip=0, loss_scale=16, train_wall=136, gb_free=20.7, wall=28369
2022-11-02 13:18:59 | INFO | train_inner | epoch 016:   3933 / 16028 loss=1.6, nll_loss=1.007, nll_loss_teacher=0.541, kd_loss=0.382, ppl=2.01, wps=34113.8, ups=0.73, wpb=46433.3, bsz=3087.2, num_updates=244300, lr=6.39791e-05, gnorm=0.181, clip=0, loss_scale=16, train_wall=136, gb_free=22.6, wall=28505
2022-11-02 13:21:14 | INFO | train_inner | epoch 016:   4033 / 16028 loss=1.586, nll_loss=0.994, nll_loss_teacher=0.542, kd_loss=0.367, ppl=1.99, wps=34489.9, ups=0.74, wpb=46564.8, bsz=3269.4, num_updates=244400, lr=6.3966e-05, gnorm=0.175, clip=0, loss_scale=16, train_wall=135, gb_free=19.5, wall=28640
2022-11-02 13:23:28 | INFO | train_inner | epoch 016:   4133 / 16028 loss=1.604, nll_loss=1.007, nll_loss_teacher=0.541, kd_loss=0.392, ppl=2.01, wps=34962.6, ups=0.74, wpb=47062.8, bsz=3126.8, num_updates=244500, lr=6.39529e-05, gnorm=0.188, clip=0, loss_scale=16, train_wall=134, gb_free=23.8, wall=28775
2022-11-02 13:25:44 | INFO | train_inner | epoch 016:   4233 / 16028 loss=1.587, nll_loss=1.001, nll_loss_teacher=0.543, kd_loss=0.362, ppl=2, wps=33882.7, ups=0.74, wpb=46085.1, bsz=2979.4, num_updates=244600, lr=6.39399e-05, gnorm=0.176, clip=0, loss_scale=16, train_wall=136, gb_free=19.7, wall=28911
2022-11-02 13:28:04 | INFO | train_inner | epoch 016:   4333 / 16028 loss=1.593, nll_loss=1.005, nll_loss_teacher=0.541, kd_loss=0.37, ppl=2.01, wps=33131.8, ups=0.72, wpb=46172.3, bsz=3221.9, num_updates=244700, lr=6.39268e-05, gnorm=0.179, clip=0, loss_scale=16, train_wall=139, gb_free=20.7, wall=29050
2022-11-02 13:30:21 | INFO | train_inner | epoch 016:   4433 / 16028 loss=1.597, nll_loss=1.002, nll_loss_teacher=0.542, kd_loss=0.382, ppl=2, wps=33464.5, ups=0.73, wpb=45788.1, bsz=3197.4, num_updates=244800, lr=6.39137e-05, gnorm=0.18, clip=0, loss_scale=16, train_wall=137, gb_free=23.6, wall=29187
2022-11-02 13:32:39 | INFO | train_inner | epoch 016:   4533 / 16028 loss=1.612, nll_loss=1.008, nll_loss_teacher=0.545, kd_loss=0.405, ppl=2.01, wps=33109.9, ups=0.72, wpb=45739.1, bsz=3053.5, num_updates=244900, lr=6.39007e-05, gnorm=0.192, clip=0, loss_scale=16, train_wall=138, gb_free=16.7, wall=29325
2022-11-02 13:34:55 | INFO | train_inner | epoch 016:   4633 / 16028 loss=1.603, nll_loss=1.006, nll_loss_teacher=0.548, kd_loss=0.389, ppl=2.01, wps=33490.5, ups=0.73, wpb=45578.9, bsz=3046.2, num_updates=245000, lr=6.38877e-05, gnorm=0.191, clip=0, loss_scale=16, train_wall=136, gb_free=24.1, wall=29461
2022-11-02 13:37:13 | INFO | train_inner | epoch 016:   4733 / 16028 loss=1.591, nll_loss=1.001, nll_loss_teacher=0.545, kd_loss=0.371, ppl=2, wps=33021.4, ups=0.72, wpb=45600, bsz=3140, num_updates=245100, lr=6.38746e-05, gnorm=0.186, clip=0, loss_scale=16, train_wall=138, gb_free=22.7, wall=29599
2022-11-02 13:39:28 | INFO | train_inner | epoch 016:   4833 / 16028 loss=1.609, nll_loss=1.001, nll_loss_teacher=0.543, kd_loss=0.406, ppl=2, wps=34031.2, ups=0.74, wpb=45885.5, bsz=3051.8, num_updates=245200, lr=6.38616e-05, gnorm=0.189, clip=0, loss_scale=16, train_wall=135, gb_free=24.7, wall=29734
2022-11-02 13:41:46 | INFO | train_inner | epoch 016:   4933 / 16028 loss=1.608, nll_loss=0.999, nll_loss_teacher=0.542, kd_loss=0.408, ppl=2, wps=33276.8, ups=0.73, wpb=45843, bsz=3167.8, num_updates=245300, lr=6.38486e-05, gnorm=0.187, clip=0, loss_scale=16, train_wall=138, gb_free=25.6, wall=29872
2022-11-02 13:44:00 | INFO | train_inner | epoch 016:   5033 / 16028 loss=1.579, nll_loss=1.003, nll_loss_teacher=0.542, kd_loss=0.346, ppl=2, wps=34327.8, ups=0.75, wpb=45979.7, bsz=3265.9, num_updates=245400, lr=6.38356e-05, gnorm=0.184, clip=0, loss_scale=16, train_wall=134, gb_free=18.1, wall=30006
2022-11-02 13:46:19 | INFO | train_inner | epoch 016:   5133 / 16028 loss=1.584, nll_loss=1.007, nll_loss_teacher=0.544, kd_loss=0.35, ppl=2.01, wps=32327.3, ups=0.72, wpb=45044.3, bsz=3192.7, num_updates=245500, lr=6.38226e-05, gnorm=0.184, clip=0, loss_scale=16, train_wall=139, gb_free=24.6, wall=30145
2022-11-02 13:48:39 | INFO | train_inner | epoch 016:   5233 / 16028 loss=1.612, nll_loss=1.004, nll_loss_teacher=0.54, kd_loss=0.409, ppl=2.01, wps=33636, ups=0.71, wpb=47075.1, bsz=3054.9, num_updates=245600, lr=6.38096e-05, gnorm=0.182, clip=0, loss_scale=16, train_wall=140, gb_free=22.2, wall=30285
2022-11-02 13:50:58 | INFO | train_inner | epoch 016:   5333 / 16028 loss=1.576, nll_loss=1.004, nll_loss_teacher=0.543, kd_loss=0.338, ppl=2, wps=32436.3, ups=0.72, wpb=45211.1, bsz=3009.8, num_updates=245700, lr=6.37966e-05, gnorm=0.178, clip=0, loss_scale=16, train_wall=139, gb_free=18.1, wall=30425
2022-11-02 13:53:13 | INFO | train_inner | epoch 016:   5433 / 16028 loss=1.605, nll_loss=1.005, nll_loss_teacher=0.543, kd_loss=0.394, ppl=2.01, wps=33640.6, ups=0.74, wpb=45461.9, bsz=3011.9, num_updates=245800, lr=6.37836e-05, gnorm=0.187, clip=0, loss_scale=16, train_wall=135, gb_free=21.3, wall=30560
2022-11-02 13:55:31 | INFO | train_inner | epoch 016:   5533 / 16028 loss=1.585, nll_loss=1.003, nll_loss_teacher=0.543, kd_loss=0.357, ppl=2, wps=32701.7, ups=0.72, wpb=45108.5, bsz=3218.2, num_updates=245900, lr=6.37706e-05, gnorm=0.184, clip=0, loss_scale=16, train_wall=138, gb_free=23, wall=30698
2022-11-02 13:57:50 | INFO | train_inner | epoch 016:   5633 / 16028 loss=1.613, nll_loss=1.01, nll_loss_teacher=0.544, kd_loss=0.406, ppl=2.01, wps=32743.8, ups=0.72, wpb=45469, bsz=3206.2, num_updates=246000, lr=6.37577e-05, gnorm=0.186, clip=0, loss_scale=16, train_wall=139, gb_free=20.6, wall=30837
2022-11-02 14:00:05 | INFO | train_inner | epoch 016:   5733 / 16028 loss=1.587, nll_loss=1.006, nll_loss_teacher=0.543, kd_loss=0.357, ppl=2.01, wps=33738.1, ups=0.74, wpb=45497.8, bsz=2976.4, num_updates=246100, lr=6.37447e-05, gnorm=0.18, clip=0, loss_scale=16, train_wall=135, gb_free=22.9, wall=30971
2022-11-02 14:02:22 | INFO | train_inner | epoch 016:   5833 / 16028 loss=1.605, nll_loss=1.013, nll_loss_teacher=0.543, kd_loss=0.388, ppl=2.02, wps=33406, ups=0.73, wpb=45859.2, bsz=2987.8, num_updates=246200, lr=6.37318e-05, gnorm=0.194, clip=0, loss_scale=16, train_wall=137, gb_free=21.6, wall=31109
2022-11-02 14:04:43 | INFO | train_inner | epoch 016:   5933 / 16028 loss=1.604, nll_loss=1.012, nll_loss_teacher=0.541, kd_loss=0.385, ppl=2.02, wps=32896.4, ups=0.71, wpb=46173.8, bsz=3036.4, num_updates=246300, lr=6.37188e-05, gnorm=0.189, clip=0, loss_scale=16, train_wall=140, gb_free=23.9, wall=31249
2022-11-02 14:07:02 | INFO | train_inner | epoch 016:   6033 / 16028 loss=1.603, nll_loss=0.998, nll_loss_teacher=0.545, kd_loss=0.398, ppl=2, wps=32508, ups=0.72, wpb=45282, bsz=3217.2, num_updates=246400, lr=6.37059e-05, gnorm=0.178, clip=0, loss_scale=16, train_wall=139, gb_free=23.9, wall=31388
2022-11-02 14:09:20 | INFO | train_inner | epoch 016:   6133 / 16028 loss=1.591, nll_loss=1.008, nll_loss_teacher=0.547, kd_loss=0.365, ppl=2.01, wps=32653.3, ups=0.72, wpb=45233.2, bsz=3001.6, num_updates=246500, lr=6.3693e-05, gnorm=0.182, clip=0, loss_scale=16, train_wall=138, gb_free=21, wall=31527
2022-11-02 14:11:38 | INFO | train_inner | epoch 016:   6233 / 16028 loss=1.603, nll_loss=1.005, nll_loss_teacher=0.539, kd_loss=0.39, ppl=2.01, wps=33801.8, ups=0.73, wpb=46553.3, bsz=3086, num_updates=246600, lr=6.36801e-05, gnorm=0.189, clip=0, loss_scale=16, train_wall=138, gb_free=23.8, wall=31665
2022-11-02 14:13:57 | INFO | train_inner | epoch 016:   6333 / 16028 loss=1.592, nll_loss=1.001, nll_loss_teacher=0.538, kd_loss=0.374, ppl=2, wps=33739.3, ups=0.72, wpb=46967.8, bsz=3128.2, num_updates=246700, lr=6.36672e-05, gnorm=0.181, clip=0, loss_scale=16, train_wall=139, gb_free=17.7, wall=31804
2022-11-02 14:16:11 | INFO | train_inner | epoch 016:   6433 / 16028 loss=1.59, nll_loss=1.01, nll_loss_teacher=0.543, kd_loss=0.36, ppl=2.01, wps=33781.1, ups=0.75, wpb=45219.2, bsz=3004, num_updates=246800, lr=6.36543e-05, gnorm=0.189, clip=0, loss_scale=16, train_wall=134, gb_free=22.9, wall=31938
2022-11-02 14:18:27 | INFO | train_inner | epoch 016:   6533 / 16028 loss=1.618, nll_loss=1.013, nll_loss_teacher=0.541, kd_loss=0.413, ppl=2.02, wps=34363.6, ups=0.74, wpb=46716.8, bsz=3029.1, num_updates=246900, lr=6.36414e-05, gnorm=0.186, clip=0, loss_scale=16, train_wall=136, gb_free=18.6, wall=32074
2022-11-02 14:20:46 | INFO | train_inner | epoch 016:   6633 / 16028 loss=1.618, nll_loss=1.009, nll_loss_teacher=0.543, kd_loss=0.418, ppl=2.01, wps=33086.7, ups=0.72, wpb=45991.1, bsz=3086, num_updates=247000, lr=6.36285e-05, gnorm=0.186, clip=0, loss_scale=16, train_wall=139, gb_free=25.8, wall=32213
2022-11-02 14:23:03 | INFO | train_inner | epoch 016:   6733 / 16028 loss=1.599, nll_loss=1.009, nll_loss_teacher=0.545, kd_loss=0.378, ppl=2.01, wps=33146.5, ups=0.73, wpb=45484.3, bsz=3081.8, num_updates=247100, lr=6.36156e-05, gnorm=0.185, clip=0, loss_scale=16, train_wall=137, gb_free=20.8, wall=32350
2022-11-02 14:25:18 | INFO | train_inner | epoch 016:   6833 / 16028 loss=1.584, nll_loss=1.014, nll_loss_teacher=0.545, kd_loss=0.345, ppl=2.02, wps=34160.7, ups=0.75, wpb=45844.1, bsz=3054.2, num_updates=247200, lr=6.36027e-05, gnorm=0.191, clip=0, loss_scale=16, train_wall=134, gb_free=20.4, wall=32484
2022-11-02 14:27:37 | INFO | train_inner | epoch 016:   6933 / 16028 loss=1.592, nll_loss=1.007, nll_loss_teacher=0.54, kd_loss=0.368, ppl=2.01, wps=32859.5, ups=0.72, wpb=45690.4, bsz=3059.8, num_updates=247300, lr=6.35899e-05, gnorm=0.188, clip=0, loss_scale=16, train_wall=139, gb_free=18.7, wall=32623
2022-11-02 14:29:46 | INFO | train_inner | epoch 016:   7033 / 16028 loss=1.604, nll_loss=1, nll_loss_teacher=0.545, kd_loss=0.398, ppl=2, wps=35088.9, ups=0.77, wpb=45332.1, bsz=3044.8, num_updates=247400, lr=6.3577e-05, gnorm=0.185, clip=0, loss_scale=16, train_wall=129, gb_free=26.3, wall=32752
2022-11-02 14:31:56 | INFO | train_inner | epoch 016:   7133 / 16028 loss=1.599, nll_loss=1.011, nll_loss_teacher=0.541, kd_loss=0.378, ppl=2.02, wps=35326, ups=0.77, wpb=45891.5, bsz=3180.8, num_updates=247500, lr=6.35642e-05, gnorm=0.189, clip=0, loss_scale=16, train_wall=130, gb_free=26.2, wall=32882
2022-11-02 14:34:04 | INFO | train_inner | epoch 016:   7233 / 16028 loss=1.607, nll_loss=1.01, nll_loss_teacher=0.547, kd_loss=0.394, ppl=2.01, wps=36341.3, ups=0.78, wpb=46659, bsz=3189.1, num_updates=247600, lr=6.35513e-05, gnorm=0.19, clip=0, loss_scale=16, train_wall=128, gb_free=26.6, wall=33011
