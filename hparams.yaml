activations: Tanh
batch_size: 16
class_identifier: regression_metric
dropout: 0.1
encoder_learning_rate: 1.0e-05
encoder_model: XLM-RoBERTa
final_activation: null
hidden_sizes:
- 3072
- 1536
keep_embeddings_frozen: true
layer: mix
layerwise_decay: 0.95
learning_rate: 3.0e-05
load_weights_from_checkpoint: wmt20-comet-da/checkpoints/model.ckpt
nr_frozen_epochs: 1
optimizer: AdamW
pool: avg
pretrained_model: xlm-roberta-large
train_data: final_data/train_mqm.csv
validation_data: final_data/test.csv